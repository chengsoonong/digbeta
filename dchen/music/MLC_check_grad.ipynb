{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-label classification -- p-classification loss check grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext line_profiler\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os, sys, time\n",
    "import numpy as np\n",
    "\n",
    "from scipy.optimize import check_grad\n",
    "from scipy.sparse import issparse, csc_matrix, csr_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('src/')\n",
    "sys.path.append('src/models')\n",
    "#from MLC import objective, risk_pclassification, DataHelper\n",
    "#from NSR import objective_clf, risk_pclassification, DataHelper, obj_clf_loop\n",
    "#from MTC import objective, risk_pclassification, DataHelper\n",
    "from MTC_L1 import objective, risk_pclassification, DataHelper\n",
    "from tools import create_dataset, dataset_names, nLabels_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ix = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = dataset_names[data_ix]\n",
    "nLabels = nLabels_dict[dataset_name]\n",
    "print(dataset_name, nLabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'data'\n",
    "SEED = 918273645"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, Y_train = create_dataset(dataset_name, train_data=True, shuffle=True, random_state=SEED)\n",
    "X_test,  Y_test  = create_dataset(dataset_name, train_data=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature normalisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_mean = np.mean(X_train, axis=0).reshape((1, -1))\n",
    "X_train_std = np.std(X_train, axis=0).reshape((1, -1)) + 10 ** (-6)\n",
    "X_train -= X_train_mean\n",
    "X_train /= X_train_std\n",
    "X_test  -= X_train_mean\n",
    "X_test  /= X_train_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_dataset_info(X_train, Y_train, X_test, Y_test):\n",
    "    N_train, D = X_train.shape\n",
    "    K = Y_train.shape[1]\n",
    "    N_test = X_test.shape[0]\n",
    "    print('%-45s %s' % ('Number of training examples:', '{:,}'.format(N_train)))\n",
    "    print('%-45s %s' % ('Number of test examples:', '{:,}'.format(N_test)))\n",
    "    print('%-45s %s' % ('Number of features:', '{:,}'.format(D)))\n",
    "    print('%-45s %s' % ('Number of labels:', '{:,}'.format(K)))\n",
    "    avgK_train = np.mean(np.sum(Y_train, axis=1))\n",
    "    avgK_test  = np.mean(np.sum(Y_test, axis=1))\n",
    "    print('%-45s %.3f (%.2f%%)' % ('Average number of positive labels (train):', avgK_train, 100*avgK_train / K))\n",
    "    print('%-45s %.3f (%.2f%%)' % ('Average number of positive labels (test):', avgK_test, 100*avgK_test / K))\n",
    "    #print('%-45s %.4f%%' % ('Average label occurrence (train):', np.mean(np.sum(Y_train, axis=0)) / N_train))\n",
    "    #print('%-45s %.4f%%' % ('Average label occurrence (test):', np.mean(np.sum(Y_test, axis=0)) / N_test))\n",
    "    print('%-45s %.3f%%' % ('Sparsity (percent) (train):', 100 * np.sum(Y_train) / np.prod(Y_train.shape)))\n",
    "    print('%-45s %.3f%%' % ('Sparsity (percent) (test):', 100 * np.sum(Y_test) / np.prod(Y_test.shape)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('%-45s %s' % ('Dataset:', dataset_name))\n",
    "print_dataset_info(X_train, Y_train, X_test, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "check gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false\n",
    "PU = np.zeros((Y_train.shape[0], 3), dtype=Y_train.dtype)\n",
    "PU[[0, 1, 2, 10], [0, 1, 1, 2]] = 1\n",
    "upl_ix = [[2, 3, 4], [5, 6, 7, 8, 9], [10, 11], [12, 13, 14, 15]]\n",
    "w0 = 0.001 * np.random.randn((Y_train.shape[1] + 3) * X_train.shape[1] + 1)\n",
    "loss = 'both'\n",
    "check_grad(\\\n",
    "lambda w: obj_pclassification(w, X_train, Y_train, C1=10, C2=1, C3=2, p=3, loss_type=loss,\n",
    "                              PU=PU, user_playlist_indices=upl_ix)[0], \n",
    "lambda w: obj_pclassification(w, X_train, Y_train, C1=10, C2=1, C3=2, p=3, loss_type=loss,\n",
    "                              PU=PU, user_playlist_indices=upl_ix)[1],w0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%script false\n",
    "cliques = [[2, 3, 4], [5, 6, 7, 8, 9], [10, 11], [12, 13]]\n",
    "#cliques = None\n",
    "w0 = 0.001 * np.random.randn(Y_train.shape[1] * X_train.shape[1] + 1)\n",
    "#w0 = np.zeros(Y_train.shape[1] * X_train.shape[1] + 1)\n",
    "dw = np.zeros_like(w0)\n",
    "loss = 'example'\n",
    "bs=5 if loss == 'label' else 100\n",
    "Y_train = csr_matrix(Y_train)\n",
    "data_helper_example = None if loss == 'label' else DataHelper(Y_train, ax=0, batch_size=bs)\n",
    "data_helper_label = None if loss == 'example' else DataHelper(Y_train, ax=1, batch_size=bs)\n",
    "#%lprun -f accumulate_risk \\\n",
    "#%lprun -f objective \\\n",
    "check_grad(lambda w: objective(w, dw, X_train, Y_train, C1=10, C2=1, C3=2, p=3, loss_type=loss, cliques=cliques, \\\n",
    "                               data_helper_example=data_helper_example, data_helper_label=data_helper_label), \\\n",
    "           lambda w: dw, w0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%script false\n",
    "cliques = [[2, 3, 4], [5, 6, 7, 8, 9], [10, 11], [12, 13]]\n",
    "#cliques = None\n",
    "w0 = 0.001 * np.random.randn(Y_train.shape[1] * (X_train.shape[1] + 1))\n",
    "#w0 = np.zeros(Y_train.shape[1] * (X_train.shape[1] + 1))\n",
    "dw = np.zeros_like(w0)\n",
    "bs=5\n",
    "Y_train = csr_matrix(Y_train)\n",
    "data_helper = DataHelper(Y_train, ax=1, batch_size=bs)\n",
    "#%lprun -f accumulate_risk \\\n",
    "#%lprun -f objective \\\n",
    "check_grad(lambda w: objective(w, dw, X_train, Y_train, C1=10, C3=2, p=3, \\\n",
    "                               cliques=cliques, data_helper=data_helper), \\\n",
    "           lambda w: dw, w0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_grad_loop(obj, grad, w0):\n",
    "    eps = 1.49e-08\n",
    "    w = np.zeros_like(w0)\n",
    "    for i in range(len(w0)):\n",
    "        if (i+1) % 10 == 0:\n",
    "            sys.stdout.write('\\r%d / %d' % (i+1, len(w0)))\n",
    "        wi1 = w0.copy()\n",
    "        wi2 = w0.copy()\n",
    "        wi1[i] = wi1[i] - eps\n",
    "        wi2[i] = wi2[i] + eps\n",
    "        J1 = obj(wi1)\n",
    "        J2 = obj(wi2)\n",
    "        w[i] = (J2 - J1) / (2 * eps)\n",
    "        #print(w[i])\n",
    "    w1 = grad(w0)\n",
    "    diff = w1 - w\n",
    "    return np.sqrt(np.dot(diff, diff))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%script false\n",
    "# objective(w, dw, X, Y, C, p, cliques, data_helper, UF=None, njobs=1, verbose=0, fnpy=None)\n",
    "\n",
    "cliques = [[0], [1], [2, 3, 4], [5, 6, 7, 8, 9], [10, 11], [12, 13]]\n",
    "#cliques = [[0], [1, 2], [3, 4, 5]]\n",
    "Ycsc = csc_matrix(Y_train)\n",
    "data_helper = DataHelper(Ycsc, cliques)\n",
    "\n",
    "UF = np.zeros((X_train.shape[0], len(cliques)))\n",
    "for u in range(len(cliques)):\n",
    "    clq = cliques[u]\n",
    "    UF[:, u] = Ycsc[:, clq].sum(axis=1).A.reshape(-1)\n",
    "    UF_mean = np.mean(UF, axis=0).reshape((1, -1))\n",
    "    UF_std = np.std(UF, axis=0).reshape((1, -1)) + 10 ** (-6)\n",
    "    UF -= UF_mean\n",
    "    UF /= UF_std\n",
    "w0 = 0.001 * np.random.randn((len(cliques) + Y_train.shape[1] + 1) * (X_train.shape[1] + len(cliques) - 1))\n",
    "\n",
    "#UF = None\n",
    "#w0 = 0.001 * np.random.randn((len(cliques) + Y_train.shape[1] + 1) * X_train.shape[1])\n",
    "\n",
    "dw = np.zeros_like(w0)\n",
    "\n",
    "#%lprun -f accumulate_risk \\\n",
    "#%lprun -f objective \\\n",
    "check_grad(lambda w: objective(w, dw, X_train, Y_train, C=10, p=3, cliques=cliques, \\\n",
    "                               data_helper=data_helper, UF=UF), \\\n",
    "           lambda w: dw, w0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false\n",
    "check_grad_loop(lambda w: objective(w, dw, X_train, Y_train, C=10, p=3, cliques=cliques, \\\n",
    "                                    data_helper=data_helper, UF=UF), \\\n",
    "                lambda w: dw, w0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#%%script false\n",
    "# objective(w, dw, X, Y, p, cliques, data_helper, verbose=0, fnpy=None)\n",
    "\n",
    "cliques = [[0], [1], [2, 3, 4], [5, 6, 7, 8, 9], [10, 11], [12, 13]]\n",
    "#cliques = [[0], [1, 2], [3, 4, 5]]\n",
    "Ycsc = csc_matrix(Y_train)\n",
    "data_helper = DataHelper(Ycsc, cliques)\n",
    "\n",
    "w0 = 0.001 * np.random.randn((len(cliques) + Y_train.shape[1] + 1) * X_train.shape[1])\n",
    "dw = np.zeros_like(w0)\n",
    "\n",
    "#%lprun -f accumulate_risk \\\n",
    "#%lprun -f objective \\\n",
    "check_grad(lambda w: objective(w, dw, X_train, Y_train, p=6, cliques=cliques, data_helper=data_helper), \\\n",
    "           lambda w: dw, w0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import pickle as pkl\n",
    "pkldir = 'data/%s/setting1' % dataset_name\n",
    "pkl.dump(X_train, gzip.open(os.path.join(pkldir, 'X_train.pkl.gz'), 'wb'))\n",
    "pkl.dump(csc_matrix(Y_train), gzip.open(os.path.join(pkldir, 'Y_train.pkl.gz'), 'wb'))\n",
    "pkl.dump(cliques, gzip.open(os.path.join(pkldir, 'cliques_train.pkl.gz'), 'wb'))\n",
    "pkl.dump(X_test, gzip.open(os.path.join(pkldir, 'X_dev.pkl.gz'), 'wb'))\n",
    "pkl.dump(csc_matrix(Y_test), gzip.open(os.path.join(pkldir, 'Y_dev.pkl.gz'), 'wb'))\n",
    "pkl.dump(cliques, gzip.open(os.path.join(pkldir, 'cliques_trndev.pkl.gz'), 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false\n",
    "print('%-20s  %-20s' % ('J1-J2', '|G1-G2|'))\n",
    "#cliques = [[0], [1], [2, 3, 4], [5, 6, 7, 8, 9], [10, 11], [12, 13]]\n",
    "cliques = [np.arange(Y_train.shape[1])]\n",
    "Y_train = csc_matrix(Y_train)\n",
    "data_helper = DataHelper(Y_train, cliques)\n",
    "for i in range(10):\n",
    "    w0 = 0.001 * np.random.randn((len(cliques) + Y_train.shape[1] + 1) * X_train.shape[1])\n",
    "    dw1 = np.zeros_like(w0)\n",
    "    dw2 = np.zeros_like(w0)\n",
    "    J1 = objective_clf(w0, dw1, X_train, Y_train, C=10, p=3, cliques=cliques, data_helper=data_helper)\n",
    "    J2 = obj_clf_loop(w0, dw2, X_train, Y_train, C=10, p=3, cliques=cliques, data_helper=data_helper)\n",
    "    print('%-20s  %-20s' % ('%g' % (J1-J2), '%g' % np.sqrt(np.dot(dw1-dw2, dw1-dw2))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "from scipy.optimize import check_grad\n",
    "\n",
    "#m, n, d = 100, 50, 20\n",
    "m, n, d = 1500, 14, 103\n",
    "X = np.random.randn(m, d)\n",
    "w0 = 0.001 * np.random.randn(n, m).reshape(-1)\n",
    "\n",
    "def obj(w):\n",
    "    assert w.shape == (n * m,)\n",
    "    W = w.reshape(n, m)\n",
    "    T = W.sum(axis=0)\n",
    "    return T.dot(X).dot(X.T).dot(T) / 2\n",
    "\n",
    "def grad(w):\n",
    "    assert w.shape == (n * m,)\n",
    "    W = w.reshape(n, m)\n",
    "    T = W.sum(axis=0)\n",
    "    Tg = X.dot(X.T).dot(T).reshape(-1)\n",
    "    return np.tile(Tg, (n, 1)).ravel()\n",
    "\n",
    "from ad import gh\n",
    "jac, hessian = gh(obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_grad(obj, jac, w0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_grad(obj, grad, w0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_grad_loop(obj, grad, w0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
