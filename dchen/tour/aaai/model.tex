%!TEX root = main.tex
%\secmoveup
\section{A structured prediction approach to sequence recommendation}
%\section{Sequence recommendation via structured prediction}
\label{sec:recseq}
%\textmoveup

We now describe a method to
solve sequence recommendation problems, such as trajectory recommendation.
We first provide background on structured SVMs, % (Sec~\ref{ssec:ssvm}),
then present a model for structured recommendation, % (Sec~\ref{ssec:sr}),
followed by the correspondingly updated algorithms for its learning % (Sec~\ref{ssec:training})
and inference. % (Sec~\ref{ssec:testing}).


%\secmoveup
\subsection{Background: SSVM for structured prediction}
\label{ssec:ssvm}
%\textmoveup

Recall that structured prediction involves predicting a structured label $\y \in \mathcal{Y}$ for an input $\x \in \mathcal{X}$,
via a score function $f(\x,\y)$.
%A popular model is the Structured Support Vector Machine (SSVM)~\cite{taskar2004max,tsochantaridis2004support}, comprising three core ingredients.
A popular approach is the Structured Support Vector Machine (SSVM)~\cite{taskar2004max,tsochantaridis2004support}, 
comprising three key ingredients.

\subsubsection{Score function}
In SSVMs, %we specify that 
the score function $f(\x, \y)$ takes a linear form, \ie
$f(\x, \y) = \w^\top \Psi(\x, \y)$,
where $\w$ is a weight vector, and $\Psi(\x, \y)$ is a \emph{joint feature map}.
The design of the feature map $\Psi$ is problem specific.
For sequence prediction problems,
%%where $\y = y_{1 : l}$
%For many problems,
%we may assume that it
%%a popular choice
%%is a vector whose elements represent
one may
consider the unary
terms for each element in the label $y_{1:l}$, and pairwise interactions between
adjacent elements in the label,
%For sequence data, %in particular,
%we also
% We may further
% assume that the pairwise interactions are between
% adjacent elements,
i.e. $y_j$ and $y_{j+1}$ for $j=1 \! : \! l \!-\! 1$.
Subsequently, $f(\x,\y)$ decomposes into a weighted sum of
each of these features: %with the corresponding feature weight:
\begin{equation}
\label{eq:jointfeature}
\resizebox{0.909\linewidth}{!}{$\displaystyle
f(\x, \y) = %\w^\top \Psi(\x, \y) =
\sum_{j=1}^l \w_j^\top \psi_j(\x, y_j) + \sum_{j=1}^{l-1} \w_{j,j+1}^\top \psi_{j,j+1}(\x, y_{j}, y_{j+1}).
$}
%\eqmoveup
\end{equation}
Here, $\psi_j$ is a feature map between the input and one output label element $y_j$, with a corresponding weight vector $\w_j$,
and $\psi_{j,j+1}$ is a pairwise feature vector that captures the interactions between consecutive labels $y_j$ and $y_{j+1}$,
with a corresponding weight vector $\w_{j,j+1}$.

%To learn the parameters, we train the structured SVM by optimising a quadratic program (QP),
\subsubsection{Objective function}
To learn a suitable weight vector $\w$ from input $\{\x\pb{i}, \y\pb{i}\}_{i=1}^n$,
SSVMs solve: %the following optimisation problem:
\begin{equation}
\label{eq:nslack}
\resizebox{0.909\linewidth}{!}{$\displaystyle
\begin{aligned}
& \min_{\w, \, \bm{\xi} \ge 0} \ \frac{1}{2} \w^\top \w + \frac{C}{n} \sum_{i=1}^n \xi_i, 
\ s.t. \ \forall i, \\
& \ \w^\top \Psi(\x^{(i)}, \y^{(i)}) - \w^\top \Psi(\x^{(i)}, \bar{\y}) \ge
  \Delta(\y^{(i)}, \bar{\y}) - \xi_i, \, \forall \bar\y \in \mathcal{Y}.
\end{aligned}
$} %\eqmoveup
\end{equation}

Here,
$\mathcal{Y}$ is the set of all possible sequences,
$\Delta(\y\pb{i}, \bar\y)$ is the loss function between $\bar\y$ and the ground truth $\y\pb{i}$,
and slack variable $\xi_i$ is the \emph{hinge loss} for the prediction of the $i$-th example. %~\cite{tsochantaridis2004support}.
Alternatively, we can use \emph{one} slack variable to represent the sum of the $n$ hinge losses in~(\ref{eq:nslack}),
\ie the 1-slack formulation~\cite{joachims2009predicting}.
%%To solve problem (\ref{eq:nslack}),
% one option is to simply enumerate all constraints, and feed the problem into a standard solver.
% However, this approach is impractical as there is a constraint for every possible label $\bar{\y}$.
%recently developed learning schemes such as
%%cutting-plane algorithms~\cite{joachims2009predicting}, % cutting-plane
%%gradient-based algorithms~\cite{ratliff2006subgradient} % sub-gradient
%%and conditional gradient methods~\cite{lacoste2013block} % Frank-Wolfe
%can efficiently optimise the objective and
%%are widely used in practice~\cite{muller2014methods}.
%LX - do we want to say the gradient-based methods are inefficient??
%Instead, we can resort to a cutting-plane algorithm which repeatedly solves the quadratic program (\ref{eq:nslack})
%with a growing set of constraints~\cite{joachims2009predicting}.
%In each iteration, a new constraint is formed by solving the loss-augmented inference,
%which helps shrink the feasible region of the problem.


\subsubsection{Loss-augmented inference}
We can rewrite the constraints in (\ref{eq:nslack}) as
$\max_{\bar\y \in \mathcal{Y}} \left\{ \Delta(\y\pb{i}, \bar\y) + \w^\top \Psi(\x\pb{i}, \bar\y) \right\} \le
\w^\top \Psi(\x\pb{i}, \y\pb{i}) + \xi_i$.
The maximisation at the left side is known as \emph{loss-augmented inference}.
%refers to the inner maximisation in Eq.~(\ref{eq:hingeloss}).
%This formulation is called "$n$-slack" as we have one slack variable for each example in training set.
%Here the inner maximisation is known as the \emph{loss-augmented inference}.
When the underlying graph of SSVM is a tree (which is the case for sequence recommendation),
this inference may be solved efficiently if the loss function $\Delta(\cdot,\cdot)$ is decomposable
with respect to individual and pairs of label elements,
\eg using the Viterbi algorithm~\cite{joachims2009predicting}.
% in a way similar to Equation~\eqref{eq:jointfeature}.

%\secmoveup
%\subsection{SSVM for structured recommendation: the SP and SR models}
\subsection{SSVM for sequence recommendation}
\label{ssec:sr}
%\textmoveup

We now present two possible means of applying SSVMs to sequence recommendation.

\subsubsection{The SP model}
Recall that sequence recommendation
involves observing \emph{multiple} ground truth outputs for each input, \ie
%If we observed more than one labels for a particular set of features,
%The classic SSVM described in Section~\ref{ssec:ssvm} can be generalised to capture this setting:
for input $\x\pb{i}$, there is a set of ground truths $\{\y\pb{ij}\}_{j=1}^{n_i}$.
%where $n_i$ is the number of labels for $\x^{(i)}$.
% describe duplicating examples
One na\"{i}ve approach to the problem
is creating
$n_i$ examples $\{(\x\pb{i}, \y\pb{ij})\}_{j=1}^{n_i}$,
and feeding this to the classic SSVM. %described in Section~\ref{ssec:ssvm} to capture this setting
We call this the \emph{structured prediction} (\emph{SP}) model.
%
The SP model is appealing due to its simplicity.
However, it is sub-optimal:
%The problem with this approach is in the result of loss-augmented inference,
%\ie
%%
%% DW: no need to exclude \yik
%%the result of loss-augmented inference on $(\xii, \yij)$ could be any label $\yik \in \{\yij\}_{j=1}^{n_i} \setminus \{\yij\}$,
%%but then we would incorrectly penalise predicting $\yik$ for $\xii$.
%%
the result of loss-augmented inference on $(\x\pb{i}, \y\pb{ij})$ could be a ground truth label $\y' \in \{\y\pb{ij}\}_{j=1}^{n_i}$,
which means we would incorrectly penalise predicting $\y'$ for $\x\pb{i}$.

\subsubsection{The SR model}
To overcome the limitation of the SP model,
we propose the following \emph{structured recommendation} (\emph{SR}) model that extends the SSVM to explicitly incorporate multiple ground truths: %for each example.
%We can learn an \emph{SR} model by optimising %a quadratic program similar to (\ref{eq:nslack}),
\begin{equation}
\label{eq:nslack_ml}
\resizebox{0.909\linewidth}{!}{$
\begin{aligned}
& \min_{\w, \, \bm{\xi} \ge 0} \ \frac{1}{2} \w^\top \w + \frac{C}{N} \sum_{i=1}^n \sum_{j=1}^{n_i} \xi_{ij},
\ s.t. \ \forall i, \, \forall j, \\
& \ \w^\top \Psi(\x^{(i)}, \y^{(ij)}) - \w^\top \Psi(\x^{(i)}, \bar{\y}^{(i)}) \ge
  \Delta(\y^{(ij)}, \bar{\y}^{(i)}) - \xi_{ij}.
\end{aligned}
$}
\end{equation}
where $N = \sum_i n_i$ and $\bar{\y}^{(i)} \in \mathcal{Y} \setminus \{\y^{(ij)}\}_{j=1}^{n_i}$.
The 1-slack formulation of~(\ref{eq:nslack_ml}) can be formed similar to that of~(\ref{eq:nslack}).
Intuitively, the objective in (\ref{eq:nslack_ml}) resembles a ranking objective, as the constraints enforce
that the positively labeled sequences 
%(the known items that the user likes) 
(items that the user likes) 
are scored higher than all other unseen sequences.
Such objectives have been proven useful in classic unstructured recommendation~\cite{bpr09}.
%Recent work on positive and unlabelled data have
%theoretically shown the close relationship between positive and unlabelled learning and two class classification.

Compared to the SP model (\ref{eq:nslack}), the key distinction is that (\ref{eq:nslack_ml})
explicitly \emph{aggregates} all the ground truth sequences for each input when generating the constraints,
\ie the loss-augmented inference becomes
\begin{equation}
\label{eq:loss_aug_inf}
\resizebox{0.909\linewidth}{!}{$
%\max_{\bar{\y}^{(i)} \in \ \mathcal{Y} \setminus \{\y^{(ij)}\}_{j=1}^{n_i}}
\underset{\bar{\y}^{(i)} \in \ \mathcal{Y} \setminus \{\y^{(ij)}\}_{j=1}^{n_i}}{\max}
     \left( \Delta(\y^{(ij)}, \bar{\y}^{(i)}) + \w^\top \Psi(\x^{(i)}, \bar\y^{(i)}) \right).
$}
\end{equation}
In this way, we do not have contradictory constraints where
two ground truth sequences are each required to have larger score than the other.

%%As a remark, we note that instead of using $N$ slack variables as that in (\ref{eq:nslack_ml}),
%%we can use \emph{one} slack variable to represent the sum of the $N$ hinge losses. % in (\ref{eq:nslack_ml}).
%which leads to a $1$-slack formulation
%This $1$-slack formulation can be learned more efficiently than the $N$-slack formulation (\ref{eq:nslack_ml}) if training using cutting-plane algorithm.
%%This 1-slack formulation (see supplement) can be learned more efficiently than (\ref{eq:nslack_ml})
%%when training using cutting-plane algorithms. %the above can be learned more efficiently than (\ref{eq:nslack_ml}).

The SP and SR models can be learned using a rich set of training schemes such as
cutting-plane algorithms~\cite{joachims2009predicting}, % cutting-plane
gradient-based algorithms~\cite{ratliff2006subgradient} % sub-gradient
and conditional gradient methods~\cite{lacoste2013block}, % Frank-Wolfe
given proper loss augmented inference and prediction procedures,
which we describe subsequently. %in the next two sections.

%DW: do we need this paragraph?
%LX - the sentence below contains little info
%To describe how to use the above models for sequence recommendation, we must precisely describe how one performs training and prediction using them.
Beyond dealing with multiple ground truths,
one additional requirement is for the predicted sequence to be a {\em path}.
This is a {\em global} constraint bringing new challenges for prediction and loss-augmented inference.
%that makes the underlying graph structure no longer a tree,
The following two sections describe the training and prediction with these requirements.
%This is the focus of the next two sections.

\subsection{SP and SR model training}
\label{ssec:training}

The main challenge in training the SP and SR models is performing loss-augmented inference~(\ref{eq:loss_aug_inf}).
As noted above, the SP model can be trained as per the vanilla SSVM.
The SR model, however, requires modifying the training procedure to account for the existence of multiple ground truths.
In particular, (\ref{eq:loss_aug_inf}) needs to be solved by \emph{excluding the sequences} $\{\y\pb{ij}\}_{j=1}^{n_i}$, and ideally with {\sc path} constraints.
We show in the following how to address both problems with an extension of the Viterbi algorithm.

%% DW: do we need to describe this here?
%%
%%For both models, a further, subtler desiderata is to
%%\emph{avoid sequences that have loops} in loss-augmented inference.
%%The motivation is that, as mentioned in \S\ref{sec:trajrec}, repeated elements are often undesirable in sequence recommendation
%%\eg a user will likely not want to visit the same location twice.


\subsubsection{List Viterbi algorithms}
List Viterbi represents a family of algorithms originally invented to decode digital signals corrupted by noise~\cite{seshadri1994list}, or to find more than one candidate sentence in speech recognition~\cite{soong1991tree}. Given a score function that can be decomposed into unary and pairwise costs such as (\ref{eq:jointfeature}), a list Viterbi algorithm finds the $k$ highest scoring sequences. % that has the highest score.
\eat{%% removed to shorten the narrative
The \emph{parallel list Viterbi} algorithm~\cite{seshadri1994list} finds the top $k$ sequences
by keeping $k$ backtrack pointers for each possible state in each position of the sequence.
This algorithm is memory-inefficient and can be difficult to use when $k$ is unknown \emph{a-priori} %one does not know what $k$ to use apriori 
-- this is the case for both the learning and inference challenges in this work.
}

A particularly applicable variant is the \emph{serial list Viterbi} algorithms (SLVA)~\cite{seshadri1994list,nilsson2001sequentially}, when the number of sequences to find is unknown a priori. 
This algorithm sequentially find the $k$-th best sequence given the best, 2nd best, \dots, $(k \!-\! 1)$-th best sequences.
%The key insight here is 
\citet{seshadri1994list} observed 
that the 2nd best sequence deviates from the best sequence
for one continuous segment, and then merges back %to the best sequence
without deviating again
-- otherwise replacing one of the continuous segments with those from the best sequence will increase the score.
Subsequently, the $k$-th best sequence can be the 2nd best sequence relative to the $(k \!-\! 1)$-th best sequence
at the point of final merge, or the 2nd or 3rd best sequence to the $(k \!-\! 2)$-th best sequence at the point of final merge, \ldots, and so on. 
\citet{nilsson2001sequentially} observed that the same goal can be achieved by cleverly partitioning the search space into subsets of sequences that share a prefix with the current list of best sequences. While derived in different communities, it is not hard to show that these two algorithms are in fact fundamentally identical.
%SLVA works by keeping track of the score differences to the best sequences and merge points along the sequences. 
See supplement for a complete description.

We use SLVA in two different ways, first to deal with multiple ground truths, and second to find paths.


\subsubsection{Eliminating multiple ground truths}
Recall that standard loss-augmented inference for an SSVM (for sequences) may be done with the classic Viterbi algorithm, but the best-scoring sequence could be in the ground-truth set $\{\y^{(ij)}\}_{j=1}^{n_i}$. We check whether this is the case, and if it is, use SLVA to decode the next best sequence until we find one that is not in the ground truth set.
%The serial list Viterbi allows exclusion of sequences with repeats,
%as well as
%excluding an arbitrary number of known sequences.
%The list Viterbi algorithm is fully detailed in the supplement.
Note that the serial list Viterbi algorithm
can be used for loss-augmented inference with Hamming loss, the most common loss function for sequence prediction tasks,
since it only requires the loss function be decomposable with respect to the label elements.


\subsubsection{Eliminating loops: \textsc{SPpath} and \textsc{SRpath}}
%Having seen the viability of the list Viterbi algorithm for removing known sequences from consideration during inference,
The list Viterbi algorithm can also enforce that loss-augmented inference only considers sequences that are \emph{paths}. This may be done by checking if each of the next-best sequences has a loop, and following SLVA as above.
This idea can be applied to both the SP and SR models, as enforcing path constraints is independent of excluding multiple ground truths.
We call the resulting models \textsc{SPpath} and \textsc{SRpath} respectively.

%LX - remark, doesn't have to be here. intuition for the real extension to SLVA?
Note that $k$ is expected to be small in noisy signal recovery, such as digital communications~\cite{seshadri1994list} and speech recognition~\cite{soong1991tree}. But this is not necessarily the case for inference in SR models.

%\secmoveup
\subsection{SP and SR model prediction}
\label{ssec:testing}
%\textmoveup

For both the SP and SR models, prediction requires that we compute $\argmax_{\y \in \mathcal{Y}_{\mathrm{path}}} f( \x, \y )$.
This is the maximum over $\mathcal{Y}_{\mathrm{path}} \subseteq \mathcal{Y}$,
which comprises all elements of $\mathcal{Y}$ that are paths. % and not all possible sequences.


\subsubsection{Eliminating loops: SLVA and ILP}
Observing that the prediction requires excluding a set of sequences from consideration, a natural and effective idea is to once again apply the {list Viterbi algorithm} of the previous section.

Alternately, we observe that our {\em path-decoding} problem is a variant of the well-known
{\em traveling salesman problem} (TSP), in particular the best of ${m \choose l}$ TSPs, 
where $m$ denotes the total number of points.
%LX - uh, do we need to explain, or is this obvious
This is because list Viterbi keeps searching the next best sequence %can find all best-scoring sequences 
until a path is discovered, %one of which is a path, 
while TSP can be formulated to find the best path.
This equivalence opens new possibilities to leverage well-studied TSP formulations
and modern implementations~\cite{tspbook2011}. In particular, we consider an integer linear program (ILP) formulation of the TSP (Eq.~\ref{eq:obj} to \ref{eq:cons4}). This formulation extends earlier work on sequence recommendation~\cite{lian2014geomf,cikm16paper} by systematically incorporating unary and pairwise scoring terms.
%LX - is the last sentence true?

%LX -- getting rid of this prelim redundant discussion
%LX - also moving Christofeds to the end of the section
%Consider the setting of finding the best path $\y^*$ that starts from a specific point $y_1^*$ and traverses exactly $l$ of $m$ candidate points.
%Given desired sequence length $l$ among $m$ possible points, the Viterbi algorithm~\cite{tsochantaridis2005large}
%will generate the length-$l$ sequence $\y^*$ with the best score. %i.e. $\y^* = \argmax_\y f(\x,\y)$.

%LX - this paragraph is logically confusing, restructure
%Such a point traversal problem can be solved by incorporating
%sub-tour elimination constraints of the TSP~\cite{ijcai15,cikm16paper}.
%In particular, %%the integer linear program (ILP) formulation (Eq.~\ref{eq:obj} to \ref{eq:cons4})
%%will solve SR inference for path $\y^*$ of length $l$ by decoding an optimal solution.
%it can be solved by decoding an optimal solution of the integer linear program (ILP) Eq.~\ref{eq:obj} to \ref{eq:cons4}.

Given a set of points $\{p_j\}_{j=1}^m$,
consider binary variables $u_{jk}$ that are true iff
the transition from $p_j$ to $p_k$ is in the resulting path,
and binary variables $z_j$ that are true iff
the optimal path $\y^*$ terminates at $p_j$.
%$p_j$ is the last point in $\y^*$.
%Suppose that $l$ is the number of candidate POIs.
For brevity, we index the points such that $y_1^* = p_1$.
Firstly, the desired path should start from $y_1^*$ (Eq.~\ref{eq:cons1}).
In addition, only $l\!-\!1$ transitions between points are permitted as the path length is $l$ (Eq.~\ref{eq:cons2}).
Moreover, any point could be visited at most once (Eq.~\ref{eq:cons3}).
The last constraint, where $v_j \in \mathbf{Z}$ is an auxiliary variable,
%enforces that $\y^*$ is a single sequence of points without sub-tours.
ensures $\y^*$ does not have disjoint cycles~\cite{Miller:1960}.
%\TODO{LX: i do not understand the last contraint, $v_j$ did not seem to have been defined? are they binary or something else?}
% DW: v_j defined in the first constraint, which is a natural number
We rewrite Eq.~(\ref{eq:jointfeature}) into a linear function of variables $u_{jk}$
(dropping the constant unary term for $y_1^*$), as % which results 
in~(\ref{eq:obj}).
%\setlength{\belowdisplayskip}{1pt} \setlength{\belowdisplayshortskip}{0pt}
%\setlength{\abovedisplayskip}{1pt} \setlength{\abovedisplayshortskip}{0pt}
%\begin{equation}%\eqmoveup\eqmoveup
%\label{eq:obj}
%\max_{\bu} ~\sum_{k=1}^m \w_k^\top \psi_k(\x, p_k) \sum_{j=1}^m u_{jk} +
%            \sum_{j,k=1}^m u_{jk} \w_{jk}^\top \psi_{j, k}(\x, p_j, p_k)
%\end{equation}
%{$\displaystyle
%\begin{minipage}{0.45\linewidth}
%\begin{alignat}{2}
%s.t. \,
%& \sum_{k=2}^m u_{1k} = 1, \, \sum_{j=2}^m u_{j1} = z_1=0                 \label{eq:cons1} \\
%& \sum_{j=1}^m \sum_{k=1}^m u_{jk} = l-1, \, \sum_{j=1}^m u_{jj}=0        \label{eq:cons2}
%\end{alignat}
%\end{minipage}
%\qquad
%\begin{minipage}{0.5\linewidth}
%\begin{alignat}{3}
%\sum_{j=1}^m u_{ji} = z_i &+ \sum_{k=2}^m u_{ik} \le 1, \, i = 2,\cdots,m  \label{eq:cons3} \\
%v_j - v_k + 1 &\le (m-1) (1-u_{jk}),                                       \nonumber \\
%              & \qquad j,k = 2,\cdots,m                                    \label{eq:cons4}
%\end{alignat}
%\end{minipage}
%$}
\vspace{-.4em}
\begin{equation}
\label{eq:obj}
\resizebox{0.909\linewidth}{!}{$\displaystyle
\max_{\bu} ~\sum_{k=1}^m \w_k^\top \psi_k(\x, p_k) \sum_{j=1}^m u_{jk} +
            \sum_{j,k=1}^m u_{jk} \w_{jk}^\top \psi_{j, k}(\x, p_j, p_k)
$}
\end{equation}
%
\begin{small}
\vspace{-1.2em}
\begin{align}
s.t. \, 
& \sum_{k=2}^m u_{1k} = 1, \, \sum_{j=2}^m u_{j1} = z_1=0                   \label{eq:cons1} \\
& \sum_{j=1}^m \sum_{k=1}^m u_{jk} = l-1, \, \sum_{j=1}^m u_{jj}=0          \label{eq:cons2} \\
& \sum_{j=1}^m u_{ji} = z_i + \sum_{k=2}^m u_{ik} \le 1, \, i = 2,\cdots,m  \label{eq:cons3} \\
& v_j - v_k + 1 \le (m-1) (1-u_{jk}), \, j,k = 2,\cdots,m                   \label{eq:cons4}
\end{align}
\end{small}

\vspace{-1.3em}

\subsubsection{Recommending more than one trajectory}
Since multiple possible trajectories can start at the same POI, 
it is desirable %we need 
to predict multiple trajectories for a query. %each test query. 
% ILP for top-k prediction/inference
%\textbf{Top-$K$ prediction}.
We can use the above ILP formulation to find the top-$K$ scored paths in a sequential manner.
In particular, given the $K\!-\!1$ top scored paths $\{\y\pb{k}\}_{k=1}^{K-1}$,
the $K\!$-th best scored path can be found by solving the above ILP with additional constraints:
%%\begin{equation}
%%\label{eq:topk_ILP}
$\sum_{j=1}^{l-1} u_{y_j, y_{j+1}} \le l-2, ~\forall \y \in \{\y^{(k)}\}_{k=1}^{K-1}$.
%%\end{equation}
Alternatively, we can achieve this by %using 
the same approach as when eliminating ground truth sequences using SLVA.


% ILP for learning (loss-augmented inference)
\subsubsection{Eliminating multiple ground truths with ILP?}
A natural question is whether one can use the above ILP
to exclude observed labels when training an SR model,
\ie solving the loss-augmented inference~(\ref{eq:loss_aug_inf}).
In fact, this can be done
%together with constraints~(\ref{eq:cons1})-(\ref{eq:cons4}),
as long as the loss $\Delta(\y,\bar\y)$ can be represented as a linear function of $u_{jk}$.
One example is the number of mis-predicted POIs disregarding the order $\Delta(\y, \bar\y) = \sum_{j=2}^l (1 - \sum_{k=1}^m u_{k, y_j})$.
However, we note that Hamming loss %%, the most common loss function for sequence prediction tasks,
%cannot be used here, 
is not applicable here,
as $\Delta(\y,\bar\y) = \sum_{j=1}^l (1 - \llb y_j = \bar y_j \rrb)$
cannot be expressed as a linear function of $u_{jk}$.


\subsubsection{Practical choices: ILP vs SLVA vs other heuristics}
When performing prediction for SP and SR model, we found that state-of-the-art ILP solvers converge to a solution faster
than the serial list Viterbi algorithm if the sequence length $l$ is large.
The reason is, while list Viterbi algorithms are polynomial time given the list depth $k$~\cite{nilsson2001sequentially},
in reality $k$ is unknown a priori and can be very large for long sequences.
We therefore use ILP for very long ($l\ge10$) sequences in the experiments (otherwise the list Viterbi algorithm is applied).

One might also consider the well-known Christofides algorithm~\cite{christofides1976} to eliminate loops in a sequence, as this is known to generate a solution within a factor of 3/2 of the optimal solution for traveling salesman problems. However, the resulting sequence will have less than the desired number of points, and its score will not be optimal.


\subsection{Discussion}
The unary terms in the sequence scoring function \eqref{eq:jointfeature} can be replaced with {\em personalised} terms to each user, such as from a recommender system~\cite{Koren:2009,bpr09}. 
We also note that the sequence recommendation problem suits recurrent neural networks (RNNs) based techniques.
However, these techniques generally require much more training data than available in our problem.
It is also not clear how to recommend a sequence as a whole instead of recommending the next best location as proposed in recent RNNs based work~\cite{aaai16}.
We leave this and personalising sequence recommendation as future work.


% \subsection{SSVM for sequence recommendation: sequence decoding}
% \label{ssec:subtour}
% \label{ssec:SRinf}

% The SR model require two algorithmic components for inference and learning
% (missing from the SSVM algorithms).
% For inference, SR needs to predict a {\em path}, \ie a sequence whose elements are distinct from each other,
% that maximises the score function (Eq.~\ref{eq:jointfeature}).
% As described in Section~\ref{sec:intro}, this is desirable since users traversing a sequence (of locations or music)
% would not want to see repeated entries.
% Given desired sequence length $l$ among $m$ possible points, the Viterbi algorithm~\cite{tsochantaridis2005large}
% will generate the length-$l$ sequence $\y^*$ with the best score. %i.e. $\y^* = \argmax_\y f(\x,\y)$.
% One may then use the well-known
% Christofides algorithm~\cite{christofides1976} on $\y^*$ to eliminate loops in the sequence.
% However, the resulting sequence will have less than the desired number of points, and the resulting score will not be optimal in general.

% For learning an SR model, loss-augmented inference (\ref{eq:loss_aug_inf}) needs to be done by excluding multiple known sequences.
% As described in Section~\ref{ssec:sr}, %this involves %we would want to maximize the loss-augmented objective function
% however, the Viterbi algorithm uses back-tracking to find the best sequence,
% and cannot easily exclude known sequences.
% The rest of this section describes two algorithms, each intuitively aimed to address %one of
% the two requirements above, which can be applied in novel contexts of the SR model.


% \textbf{Finding paths with integer programming}.
% For inference in the SR model,
% we consider the setting of finding the best path $\y^*$ that starts from a specific point $y_1^*$ and traverses exactly $l$ of $m$ candidate points.
% This can be seen as a variant of the travelling salesman problem (TSP), or the best of ${m \choose l}$ TSPs.
% Such a point traversal problem can be solved by incorporating
% sub-tour elimination constraints of the TSP~\cite{ijcai15,cikm16paper}.
% In particular, the integer linear program (ILP) formulation (Eq.~\ref{eq:obj} to \ref{eq:cons5})
% will solve SR inference for path $\y^*$ of length $l$ by decoding an optimal solution.

% Given a set of points $\{p_j\}_{j=1}^m$,
% consider binary decision variables $u_{jk}$ that are true if and only if
% the transition from $p_j$ to $p_k$ is in the resulting path,
% and binary decision variables $z_j$ that are true iff $p_j$ is the last point in $\y^*$.
% %Suppose that $l$ is the number of candidate POIs.
% For brevity, we index the points such that $y_1^* = p_1$.
% Firstly, the desired path should start from $y_1^*$ (Constraint~\ref{eq:cons1}).
% In addition, only $l\!-\!1$ transitions between points are permitted as the path length is $l$ (Constraint~\ref{eq:cons2}).
% Moreover, any point could be visited at most once (Constraint~\ref{eq:cons3}).
% The last constraint, where $v_j \in \mathbf{Z}$ is an auxiliary variable,
% enforces that $\y^*$ is a single sequence of points without sub-tours.
% %\TODO{LX: i do not understand the last contraint, $v_j$ did not seem to have been defined? are they binary or something else?}
% % DW: v_j defined in the first constraint, which is a natural number
% We rewrite Eq.~(\ref{eq:jointfeature}) into a linear function of decision variables $u_{jk}$
% (dropping the unary term corresponding to $y_1^*$ as it has been observed), which results in ~(\ref{eq:obj}):

% \begin{equation}
% \label{eq:obj}
% \max_{\bu} ~\sum_{k=1}^m \w_k^\top \psi_k(\x, p_k) \sum_{j=1}^m u_{jk} +
%             \sum_{j,k=1}^m u_{jk} \w_{jk}^\top \psi_{j, k}(\x, p_j, p_k)
% \end{equation}

% {$\displaystyle
% \begin{minipage}{0.45\linewidth}
% \begin{alignat}{2}
% s.t. \,
% & \sum_{k=2}^m u_{1k} = 1, \, \sum_{j=2}^m u_{j1} = z_1=0                 \label{eq:cons1} \\
% & \sum_{j=1}^m \sum_{k=1}^m u_{jk} = l-1, \, \sum_{j=1}^m u_{jj}=0        \label{eq:cons2}
% \end{alignat}
% \end{minipage}
% \qquad
% \begin{minipage}{0.5\linewidth}
% \begin{alignat}{3}
% \sum_{j=1}^m u_{ji} = z_i + \sum_{k=2}^m u_{ik} \le 1, \, i = 2,\cdots,m  \label{eq:cons3} \\
% v_j - v_k + 1 \le (m-1) (1-u_{jk}),                                       \nonumber \\
% j,k = 2,\cdots,m~                                                         \label{eq:cons4}
% \end{alignat}
% \end{minipage}
% $}

% We can use the above formulation to find the top-$K$ scored paths in a sequential manner.
% In particular, given the $K\!-\!1$ top scored paths $\{\y^{(k)}\}_{k=1}^{K-1}$,
% the $K\!$-th best scored path can be found by solving the above ILP with additional constraints:
% $\sum_{j=1}^{l-1} u_{y_j, y_{j+1}} \le l-2, ~\forall \y \in \{\y^{(k)}\}_{k=1}^{K-1}$.
% This constraint can also be used to exclude observed labels when learning an SR model,
% \ie solving the loss-augmented inference~(\ref{eq:loss_aug_inf}),
% together with constraints~(\ref{eq:cons1})-(\ref{eq:cons4}),
% as long as the loss $\Delta(\y,\bar\y)$ can be represented as a linear function of $u_{jk}$,
% \eg the number of mis-predicted POIs disregarding the order $\Delta(\y, \bar\y) = \sum_{j=2}^l (1 - \sum_{k=1}^m u_{k, y_j})$.
% However, we note that Hamming loss, the most common loss function for sequence prediction tasks,
% cannot be used here, as $\Delta(\y,\bar\y) = \sum_{j=1}^l (1 - \llb y_j = \bar y_j \rrb)$
% cannot be expressed as a linear function of $u_{jk}$.

% \textbf{Finding $k$-best sequences}.
% An alternative approach to solve the inference and learning of the SR model is making use of
% several extensions of the Viterbi algorithm that aim to find more than one high-scoring sequences.
% The \emph{parallel list Viterbi} algorithm~\cite{seshadri1994list} finds the top $k$ sequences
% by keeping $k$ backtrack pointers for each possible state in each position of the sequence.
% However, it is difficult to apply to the SR inference scenario,
% since we do not know $k$ beforehand.
% This is because it is generally impossible to foresee
% the rank (according to the score) of the first path among all valid sequences (that may have repeated points).

% We resort to the \emph{serial list Viterbi} algorithm~\cite{seshadri1994list,nilsson2001sequentially}.
% These algorithms sequentially find the $k$-th best sequence given the best, 2nd best, \dots, $(k \!-\! 1)$-th best sequences.
% The key insight here is that the 2nd best sequence deviates from the best sequence
% for one continuous segment, and then finally merges back to the best sequence without deviating again
% -- otherwise replacing one of the continuous segments with those from the best sequence will increase the score.
% Subsequently, the $k$th best sequence can be the 2nd best sequence relative to the $(k \!-\! 1)$-th sequence
% at the point of final merge, or the 2nd or 3rd best sequence to the $(k \!-\! 2)$-th sequence at the point of final merge, \ldots, and so on.
% %The version of serial list Viterbi presented by~\cite{nilsson2001sequentially} accommodates
% The serial list Viterbi allows exclusion of sequences with repeats, %by checking whether or not the current $k$th best sequence has a repeat, if it does, discard and proceed to the $(k \!+\! 1)$-th.
% %and also allows
% as well as
% excluding an arbitrary number of known sequences.
% %by partitioning the remaining space
% The list Viterbi algorithm is fully detailed in the supplement.

% The serial list Viterbi algorithm is particularly versatile
% for the structured recommendation problem: %the serial list Viterbi algorithm
% it can be used for inference by sequentially getting the next best sequence, until the required number of %one or more
% paths are found;
% it can also be used for loss-augmented inference with Hamming loss,
% since it only requires the loss function be decomposable with respect to the label elements.

% List Viterbi algorithms are polynomial time given the list depth $k$~\cite{nilsson2001sequentially},
% but in reality $k$ is unknown a priori and can be very large for long sequences.
