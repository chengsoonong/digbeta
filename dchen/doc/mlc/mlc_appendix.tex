\section{Multi-label classification}
\label{sec:mlc}

%1. Brief summary of reference
\paragraph{Summary}
\citet{dembczynski:2010} formalised the multi-label classification problem, 
and claimed that if Hamming loss or rank loss is used,
multi-label classification methods, in theory, could not benefit from modelling label dependence.
On the other hand, modelling correlation between labels was necessary if one chose to use the subset 0/1 loss.
Further, a probabilistic classifier chains (PCC), which generalised the classifier chains (CC) from a probabilistic perspective,
was proposed to modelling label correlation. 

Theoretically, the order of labels does not affect the model, 
in practice, however, using different order of labels will result in different model parameters (we do not have infinity data).
To alleviate this issue, an ensemble of PCC (EPCC) was proposed, which made a prediction by averaging over predictions by a number of PCCs, 
each model was trained using a randomly chosen permutation of the labels.
PCC (and EPCC) was empirically shown to outperform a number of baselines that did not model label correlations when label dependence existed in data.


\noindent
\paragraph{Definition}
Let $\LCal = \{\lambda_1,\dots,\lambda_l\}$ be a finite set of class labels,
and example $(\x,\y) \in \XCal \times \YCal$, 
where $\YCal \in \{0,1\}^m$ is the set of all possible labels,
and $\y=y_{1:m}$ is a binary vector where $y_i = 1$ \emph{iff} $\lambda_i$ is a label of $\x$.
A multi-label classifier is a mapping $\h: \XCal \to \YCal$.

\noindent
\paragraph{Label dependence}
Suppose examples are independent and identically distributed (iid) according to a joint probability distribution $\p(\X,\Y)$ on $\XCal \times \YCal$,
where $\X$ is a random variable and $\Y=Y_{1:l}$ is a random vector,
Let $\p\pb{i}(Y_i |\x)$ be the marginal distribution of $Y_i$, then
\begin{equation*}
\p\pb{i}(Y_i=b |\x) = \sum_{\y \in \YCal:y_i = b} \p(\Y = \y |\x),
\end{equation*}
where $\p(\Y = \y |\x)$ is the posterior distribution given observation $\x$.
We note that the labels are not independent if 
\begin{equation*}
\p(\Y |\x) \ne \prod_{i=1}^l \p\pb{i}(Y_i |\x),
\end{equation*}
and the degree of dependence could be quantified in terms of measures such as cross entropy and KL divergence.

\noindent
\paragraph{Learning}
Given a loss function $\ell(\cdot)$, 
we can learn a multi-label classifier by find a model $\h^*$ that minimise the expected loss over the joint distribution $\p(\X,\Y)$:
\begin{equation*}
\h^* 
= \argmin_{\h} \, \E_{\X\Y} \, \ell(\Y,\h(\X))
= \argmin_{\h} \, \E_{\X} \, \E_{\Y|\X} \, \ell(\Y,\h(X))
= \argmin_{\h} \, \sum_{\x} \x \p(\x) \, \E_{\Y|\X} \, \ell(\Y,\h(\x)),
\end{equation*}
thanks to the summation, fix $\x$, we have
\begin{equation*}
\h^*(\x) = \argmin_{\y} \, \E_{\Y|\X} \, \ell(\Y,\y).
\end{equation*}
Frequently used loss functions in the context of multi-label classification including Hamming loss, rank loss and subset 0/1 loss~\cite{dembczynski:2010},
here we focus on a rank loss (taking care of ties):
\begin{equation}
\label{eq:loss_rank}
\ell(\y, \h(\x)) = \sum_{(i,j): y_i > y_j} \left( \llb h_i < h_j \rrb + \frac{1}{2} \llb h_i = h_j \rrb \right).
\end{equation}
\emph{Theorem 3.1 in~\cite{dembczynski:2010} here.}

\noindent
\paragraph{Probabilistic classifier chains}
Given a query $\x$, the posterior probability of a label $\y$ can be computed using the product rule of probability:
\begin{equation*}
\p(\y |\x) = \p(y_1) \cdot \prod_{i=2}^l \p(y_i |\x, y_{1:i-1}),
\end{equation*}
and we further define a function:
\begin{equation*}
f_i = 
\begin{cases}
\p(y_i = 1 |\x), & i = 1 \\
\p(y_i = 1 |\x, y_{1:i-1}), & 1 < i \le l
\end{cases}
\end{equation*}
then we have
\begin{equation*}
\p(\y |\x) = f_1 \cdot \prod_{i=2}^l f_i,
\end{equation*}
where $f_i$ uses $\x$ and $y_{1:i-1}$ as the input features. 
Theoretically, the results of the product rule does not depend on the order of variables, 
however, in practice, different order of variables will result in different model parameters (\ie the order of features depend on the order of variables). \\
\emph{Greedy approach -- classifier chain; assuming Markov property, we can use the Viterbi algorithm; with Neural net, we can build an order agnostic model.}


\section{Bipartite ranking}
\label{sec:birank}

%1. Brief summary of reference
\paragraph{Summary}
\citet{li:2014} proposed a new algorithm (\ie \emph{TopPush}) for bipartite ranking to optimise the ranking accuracy at the top.
This algorithm has a linear time complexity at each iteration of the optimisation process.

The key observation was that the loss used in~\cite{agarwal:2011} (when indicator function is replaced with a convex surrogate)
can be equivalently transformed to a new form 
which can be optimised in linear time (w.r.t the size of training set).


\paragraph{Definition} 
Bipartite ranking is to learn a real-valued ranking function that places positive examples above negative examples~\cite{li:2014}.
Formally, given training examples $S = S_+ \cup S_-$ with $m$ positive examples $S_+ = \{\x_i^+\}_{i=1}^m$ and $n$ negative examples $S_- = \{\x_i^-\}_{i=1}^n$, 
bipartite ranking aims to learn a ranking function $f: \XCal \to \R$ that is likely ranks positive examples higher than negative examples.

\paragraph{Loss function}
AUC is a widely used as an evaluate metric for bipartite ranking, and it turns out that AUC can be optimised by minimising a loss defined as~\cite{cortes:2004}
\begin{equation}
\label{eq:loss_auc}
\ell_\text{rank}(f; S) = \frac{1}{mn} \sum_{i=1}^m \sum_{j=1}^n \llb f(\x_i^+) \le f(\x_j^-) \rrb,
\end{equation}
and this loss can be easily optimised (\eg by gradient descent) if we replace the indicator function with a convex surrogate such as the truncated quadratic loss 
$\ell(z) = (1+z)_+^2$, the exponential loss $\ell(z) = e^z$ and logistic loss $\ell(z) = \log(1+e^z)$.
One drawback of this loss function is enumerating all the positive-negative pairs, which is computationally expensive for large dataset. \\
\emph{Theorem 3.1 in~\cite{dembczynski:2010} for this loss function here.}

Alternatively, one may interested in optimising the ranking accuracy only at the top, 
or equivalently, we would like to minimize the number of positive examples that ranked below the highest-ranking negative instance~\cite{agarwal:2011,li:2014}:
\begin{equation}
\label{eq:loss_inf}
\begin{aligned}
\ell_{\infty}(f; S) 
&= \max_{1 \le j \le n} \frac{1}{m} \sum_{i=1}^m \, \llb f(\x_i^+) < f(\x_j^-) \rrb \\
&= \frac{1}{m} \sum_{i=1}^m \max_{1 \le j \le n} \llb f(\x_i^+) < f(\x_j^-) \rrb,
\end{aligned}
\end{equation}
by replace the indicator function in (\ref{eq:loss_inf}) with a convex surrogate $\ell(\cdot)$, we have
\begin{equation}
\label{eq:loss_inf1} 
\begin{aligned}
\tilde{\ell}_{\infty}(f; S) 
&= \frac{1}{m} \sum_{i=1}^m \max_{1 \le j \le n} \ell\left( f(\x_j^-) - f(\x_i^+) \right) \\
&= \frac{1}{m} \sum_{i=1}^m \ell\left( \max_{1 \le j \le n} f(\x_j^-) - f(\x_i^+) \right),
\end{aligned}
\end{equation}
which can be optimised more efficiently than (\ref{eq:loss_auc})~\cite{li:2014}.

\paragraph{Dual formulation}
Consider a linear ranking function $f(\x) = \w^\top \x$ and loss function~\ref{eq:loss_inf1}.
Table~\ref{tab:symbol} summarises some notation we will use.
\begin{table}[!h]
\caption{Glossary of commonly used symbols}
\label{tab:symbol}
\renewcommand{\arraystretch}{1.5} % tweak the space between rows
\setlength{\tabcolsep}{1pt} % tweak the space between columns
\centering
\begin{tabular}{llll}
\hline \hline
\multicolumn{3}{l}{\textbf{Symbol}} & \textbf{Quantity} \\ \hline 
$d$              &  $\in$  &  $\Z^+$  & The number of features for each example \\
$\w$             &  $\in$  &  $\R^d$  & The vector of model parameters \\
$\mathbf{1}_m$   &  $\in$  &  $\R^m$  & The $m$ dimensional vector of $1$'s \\
$\X^+$           &  $\in$  &  $\R^{m \times d}\quad$  & Matrix of features of positive examples \\
$\X^-$           &  $\in$  &  $\R^{n \times d}$       & Matrix of features of negative examples \\
$\alphabm$       &  $\in$  &  $\R^m$  &  Dual variables for positive examples \\
$\betabm, \nubm$ &  $\in$  &  $\R^n$  &  Dual variables for negative examples \\ \hline
\end{tabular}
\end{table}

We can learn the model parameters $\w$ by risk minimisation with L2 regularisation:
\begin{equation}
\label{eq:minrisk}
\min_{\w} \, \frac{\lambda}{2} \w^\top \w + \frac{1}{m} \sum_{i=1}^m \ell\left( \max_{1 \le j \le n} \w^\top \x_j^- - \w^\top \x_i^+ \right),
\end{equation}
where $\lambda > 0$ is a regularisation constant.
Problem (\ref{eq:minrisk}) is hard to optimise in general due to the maximum term in loss function, one widely used trick is to form its dual problem.
Let 
\begin{equation*}
\begin{aligned}
f_0 (\w, \xi) &= \frac{\lambda}{2} \w^\top \w + \frac{1}{m} \sum_{i=1}^m \ell\left( \xi - \w^\top \x_i^+ \right), \\
f_j (\w, \xi) &= \w^\top \x_j^- - \xi, \ j \in \{1,\dots,n\}.
\end{aligned}
\end{equation*}
Then problem (\ref{eq:minrisk}) is equivalent to
\begin{equation}
\label{eq:minrisk_lg}
\begin{aligned}
\min_{\w, \xi} \quad & f_0 (\w, \xi) \\
s.t. \quad & f_j (\w, \xi) \le 0, \ j \in \{1,\dots,n\}.
\end{aligned}
\end{equation}
For $\nu_j \ge 0, \, j \in \{1,\dots,n\}$, the \emph{Lagrangian} of (\ref{eq:minrisk_lg}) is
\begin{equation}
\label{eq:minrisk_lg1}
\begin{aligned}
L(\w, \xi, \nubm) 
&= f_0 (\w, \xi) + \sum_{j=1}^n \nu_j \cdot f_j(\w, \xi) \\
&= \frac{\lambda}{2} \w^\top \w + \frac{1}{m} \sum_{i=1}^m \ell\left( \xi - \w^\top \x_i^+ \right) + \sum_{j=1}^n \nu_j \cdot \left( \w^\top \x_j^- - \xi \right)
\end{aligned}
\end{equation}
Note that the conjugate of the conjugate of a convex function is itself, \ie $f(\z) = f^{**}(\z) = \sup_{\y} \left( \z^\top \y - f^*(\y) \right)$, we have
\begin{equation}
\label{eq:lg_part1}
\begin{aligned}
\frac{1}{m} \sum_{i=1}^m \ell\left( \xi - \w^\top \x_i^+ \right)
&= \frac{1}{m} \sum_{i=1}^m \sup_{\alpha_i} \left( (\xi - \w^\top \x_i^+) \cdot \alpha_i - \ell^*(\alpha_i) \right) \\
&= \sup_{\alphabm} \left[ \frac{1}{m} \sum_{i=1}^m (\xi - \w^\top \x_i^+) \cdot \alpha_i - \frac{1}{m} \sum_{i=1}^m \ell^*(\alpha_i) \right] \\
&= \sup_{\alphabm} \left[ \frac{\xi}{m} \1_m^\top \alphabm - \frac{1}{m} \alphabm^\top \X^+ \w - \frac{1}{m} \sum_{i=1}^m \ell^*(\alpha_i) \right] \\
\end{aligned}
\end{equation}
where $\ell^*(\cdot)$ is the conjugate of $\ell(\cdot)$.
Further, 
\begin{equation}
\label{eq:lg_part2}
\sum_{j=1}^n \nu_j \cdot \left( \w^\top \x_j^- - \xi \right) = \nubm^\top \X^- \w - \xi \1_n^\top \nubm
\end{equation}
Then by (\ref{eq:minrisk_lg1}), (\ref{eq:lg_part1}) and (\ref{eq:lg_part2}), we have
\begin{align*}
L(\w, \xi, \alphabm, \nubm) 
&= \frac{\lambda}{2} \w^\top \w + 
   \sup_{\alphabm} \left[ \frac{\xi}{m} \1_m^\top \alphabm - \frac{1}{m} \alphabm^\top \X^+ \w - \frac{1}{m} \sum_{i=1}^m \ell^*(\alpha_i) \right] +
   \nubm^\top \X^- \w - \xi \1_n^\top \nubm \\
&= \sup_{\alphabm} \left[ 
   \frac{\lambda}{2} \w^\top \w + 
   \frac{\xi}{m} \1_m^\top \alphabm - \frac{1}{m} \alphabm^\top \X^+ \w - \frac{1}{m} \sum_{i=1}^m \ell^*(\alpha_i) +
   \nubm^\top \X^- \w - \xi \1_n^\top \nubm \right] \\
&= \sup_{\alphabm} \left[ g(\w, \xi) - \frac{1}{m} \sum_{i=1}^m \ell^*(\alpha_i) \right]
\end{align*}
where
$$g(\w, \xi) = \frac{\lambda}{2} \w^\top \w + \frac{\xi}{m} \1_m^\top \alphabm - \frac{1}{m} \alphabm^\top \X^+ \w + \nubm^\top \X^- \w - \xi \1_n^\top \nubm$$
The \emph{Lagrangian dual function} of (\ref{eq:minrisk_lg}) is
\begin{equation}
\label{eq:lg_dual_func}
\begin{aligned}
\inf_{\w, \xi} \, L(\w, \xi, \alphabm, \nubm) 
&= \inf_{\w, \xi}  \, \sup_{\alphabm} \left[ g(\w, \xi) - \frac{1}{m} \sum_{i=1}^m \ell^*(\alpha_i) \right] \\
&= \sup_{\alphabm} \, \inf_{\w, \xi} \left[ g(\w, \xi) - \frac{1}{m} \sum_{i=1}^m \ell^*(\alpha_i) \right] ~~ \text{(assuming strong duality)} \\
&= \max_{\alphabm} \, \min_{\w, \xi} \left[ g(\w, \xi) - \frac{1}{m} \sum_{i=1}^m \ell^*(\alpha_i) \right] ~~ \text{(Equation (\ref{eq:minrisk}) is L2 regularised~\cite{shalev:2007})} \\
&= \max_{\alphabm} \left[ \min_{\w, \xi} g(\w, \xi) - \frac{1}{m} \sum_{i=1}^m \ell^*(\alpha_i) \right]
\end{aligned}
\end{equation}
To solve the (unconstrained) inner minimisation, let
\begin{align*}
\frac{\partial g}{\partial \w}  &= \lambda \w - \frac{1}{m} \left( \alphabm^\top \X^+ \right)^\top + \left( \nubm^\top \X^- \right)^\top = 0 \\
\frac{\partial g}{\partial \xi} &= \frac{1}{m} \1_m^\top \alphabm - \1_n^\top \nubm = 0
\end{align*}
Then we have
\begin{equation}
\label{eq:sol1}
\widetilde\w 
= \frac{1}{\lambda m} \left( \alphabm^\top \X^+ - m \nubm^\top \X^- \right)^\top 
= \frac{1}{\lambda m} \left( \alphabm^\top \X^+ - \betabm^\top \X^- \right)^\top  
\end{equation}
and
\begin{equation}
\label{eq:sol2}
\1_m^\top \alphabm = m \1_n^\top \nubm = \1_n^\top \betabm
\end{equation}
where $\betabm = m \nubm \succeq 0$, and by (\ref{eq:sol1}) and (\ref{eq:sol2}), we have
\begin{equation}
\label{eq:min_func}
\begin{aligned}
\min_{\w, \xi} g(\w, \xi) 
&= \frac{\lambda}{2} \widetilde\w^\top \widetilde\w + \frac{\xi}{m} \1_m^\top \alphabm -
   \frac{1}{m} \alphabm^\top \X^+ \widetilde\w + \nubm^\top \X^- \widetilde\w - \xi \1_n^\top \nubm \\
&= \frac{\lambda}{2} \widetilde\w^\top \widetilde\w + 
   \frac{\xi}{m} \left( \1_m^\top \alphabm - m \1_n^\top \nubm \right) - 
   \frac{1}{m} \left( \alphabm^\top \X^+ - m \nubm^\top \X^- \right) \widetilde\w \\
&= \frac{\lambda}{2} \widetilde\w^\top \widetilde\w + 0 - \lambda \widetilde\w^\top \widetilde\w \\
&= -\frac{\lambda}{2} \widetilde\w^\top \widetilde\w \\
&= -\frac{1}{2 \lambda m^2} \left\| \alphabm^\top \X^+ - \betabm^\top \X^- \right\|^2
\end{aligned}
\end{equation}
Lastly, by (\ref{eq:lg_dual_func}) and (\ref{eq:min_func}), the \emph{Lagrangian dual problem} of (\ref{eq:minrisk_lg}) is
\begin{align*}
\max_{\nubm} \, \inf_{\w, \xi} \, L(\w, \xi, \alphabm, \nubm) 
= \max_{\alphabm, \nubm} \, \left[ \min_{\w, \xi} g(\w, \xi) - \frac{1}{m} \sum_{i=1}^m \ell^*(\alpha_i) \right]
= \max_{\alphabm, \betabm} \, \left[ -\frac{1}{2 \lambda m^2} \left\| \alphabm^\top \X^+ - \betabm^\top \X^- \right\|^2 - 
  \frac{1}{m} \sum_{i=1}^m \ell^*(\alpha_i) \right]
\end{align*}
subject to $\1_m^\top \alphabm = m \1_n^\top \nubm$ and $\nubm \succeq 0$,
or equivalently 
\begin{equation}
\label{eq:minrisk_dual}
\begin{aligned}
\min_{\alphabm, \betabm} \quad & \frac{1}{2 \lambda m} \left\| \alphabm^\top \X^+ - \betabm^\top \X^- \right\|^2 + \sum_{i=1}^m \ell^*(\alpha_i) \\
s.t. \quad & \1_m^\top \alphabm = \1_n^\top \betabm \\
& \betabm \succeq 0.
\end{aligned}
\end{equation}


\section{Playlist generation as multi-label classification}
\label{sec:playlist}

%2. Formal problem statement (e.g. input, output)
%\subsection{Problem formulation}

Given $N$ playlists where songs in each playlist are from a music library with $K$ songs $\{s_i\}_{i=1}^K$,
we derive a training set $\SCal = \left\{ \left( \x\pb{n}, \y\pb{n} \right) \right\}_{n=1}^N$ where $\x\pb{n} \in \R^D$ is a feature vector of the query 
induced by the $n$-th playlist (\eg the feature vector of the first song in the $n$-th playlist),
$\y\pb{n} \in \{0,1\}^K$ is a binary indicator such that 
$$
y_i\pb{n} = 
\begin{cases}
1, & \text{song $s_i$ is in the $n$-the playlist} \\
0, & \text{otherwise}
\end{cases}
$$

The empirical risk of a predictor $\f$ (with parameters $\w$) on training set $\SCal$ is
\begin{equation}
\label{eq:risk_pl}
R_{\LCal}(\f; \SCal) = \frac{1}{N} \sum_{n=1}^N \LCal\left(\f(\x\pb{n}), \y\pb{n}\right),
\end{equation}
where $\LCal(\cdot)$ is a loss function for multi-label learning such as Hamming loss, rank loss, subset 0/1 loss etc.

For training example $(\x, \y)$, let $K_+ = \{i |y_i = 1\}$ and $K_- = \{j |y_j = 0\}$, 
we consider three options for $\LCal(\cdot)$ here:
\begin{enumerate}
\item Hamming loss: 
      \begin{equation}
      \label{eq:loss_hamm_pl0}
      \LCal_\text{Hamm}(\f(\x), \y) = \frac{1}{K} \sum_{i=1}^K \; \llb f_i \ne y_i \rrb
      \end{equation}
\item Rank loss: 
      \begin{equation}
      \label{eq:loss_rank_pl0}
      \LCal_\text{Rank}(\f(\x), \y) = \frac{1}{K_+ \cdot K_-} \sum_{i \in K_+} \sum_{j \in K_-} 
                                      \left( \llb f_i < f_j \rrb + \frac{1}{2} \llb f_i = f_j \rrb \right)
      \end{equation}

\item Top-push loss:
      $$\LCal_\infty(\f(\x), \y) = \frac{1}{K_+} \sum_{i \in K_+} \max_{j \in K_-} \llb f_i < f_j \rrb,$$
      let $\ell(\cdot)$ be a convex surrogate of the indicator function, we have
      \begin{equation}
      \label{eq:loss_inf_pl}
      \widetilde{\LCal}_\infty(\f(\x), \y) = \frac{1}{K_+} \sum_{i \in K_+} \ell\left( \max_{j \in K_-} f_j - f_i \right).
      \end{equation}
\end{enumerate}

To learn the parameters of predictor $\f$, we can minimise the empirical risk (\ref{eq:risk_pl}) with L2 regularisation:
\begin{equation*}
%\label{eq:minrisk_l2}
\min_{\w} \, \frac{1}{2} \w^\top \w + R_{\LCal}(\f; \SCal).
\end{equation*}


\subsection{Baselines}

%\paragraph{First song as seed +  Independent logistic regression}
%Suppose the feature of a query induced by a playlist is simply the feature of the first song in the playlist 
%(\ie use the first song in a playlist as the \emph{seed}).

Given a loss function $\LCal(\cdot)$, assuming L2 regularisation\footnote{
The regularisation constant is different from that in scikit-learn~\cite{sklearn-guide}, 
as the objective there is $J(\w) = \frac{1}{2}\w^\top \w + C \sum_{n=1}^N \LCal(\x^n, \y^n; \w)$, so we have $C = \frac{1}{N \lambda}$.},
the optimisation objective to learn weights $\w$ is
\begin{equation}
\label{eq:obj}
J(\w) = \frac{\lambda}{2}\w^\top \w + \frac{1}{N} \sum_{n=1}^N \LCal(\x^n, \y^n; \w),
\end{equation}
where $\w = [\w_1^\top, \cdots, \w_K^\top]^\top$ is the flattened weight vector.
We further assume the predicted score of a label has a \emph{linear} form, \ie $f_k(\x) = \w_k^\top \x, \, k \in \{1,\cdots,K\}$.



\subsubsection{Independent logistic regression}
\label{sssec:logistic}
The most natural idea is to make predictions for each label, by independently learning a binary classifier (\eg logistic regression) for each label,
\ie, to learn the parameters $\w_k$ for the $k$-th label, we minimise the L2 regularised log likelihood:
\begin{align*}
J(\w_k) 
&= \frac{\lambda}{2} \w_k^\top \w_k + 
   \frac{1}{N} \sum_{n=1}^N -\log \left[ \p(\hat{y}_k^n = 1 | \x^n)^{y_k^n} (1 - \p(\hat{y}_k^n = 1 | \x^n))^{(1 - y_k^n)} \right] \\
&= \frac{\lambda}{2} \w_k^\top \w_k + 
   \frac{1}{N} \sum_{n=1}^N -\log \left[ \sigma(\w_k^\top \x^n)^{y_k^n} (1 - \sigma(\w_k^\top \x^n))^{(1 - y_k^n)} \right] \\
&= \frac{\lambda}{2} \w_k^\top \w_k + 
   \frac{1}{N} \sum_{n=1}^N \left[ -y_k^n \log \sigma(\w_k^\top \x^n) - (1 - y_k^n) \log (1 - \sigma(\w_k^\top \x^n)) \right]
\end{align*}
where $\p(\hat{y}_k = 1 |\x)$ is the probability that the $k$-th label of $\x$ is positive and
$\sigma(v) = [1 + e^{-v}]^{-1}$ is the sigmoid function.
To compute the gradient, note that
$$
\frac{d \, \sigma(v)}{d \, v} = -(1 + e^{-v})^{-2} (-e^{-v}) = \frac{1}{1 + e^{-v}} \cdot \frac{e^{-v} + 1 - 1}{1 + e^{-v}} = \sigma(v) (1 - \sigma(v)),
$$
Then
\begin{align*}
\frac{\partial \, J(\w_k)} {\partial \, \w_k} 
&= \lambda \w_k + \frac{1}{N} \sum_{n=1}^N \left[ -y_k^n \frac{\sigma(\w_k^\top \x^n) (1 - \sigma(\w_k^\top \x^n)) \x^n} {\sigma(\w_k^\top \x^n)} -
   (1 - y_k^n) \frac{- \sigma(\w_k^\top \x^n) (1 - \sigma(\w_k^\top \x^n)) \x^n} {1 - \sigma(\w_k^\top \x^n)} \right] \\
&= \lambda \w_k + \frac{1}{N} \sum_{n=1}^N \left[ -y_k^n \x^n (1 - \sigma(\w_k^\top \x^n)) + (1 - y_k^n) \x^n \sigma(\w_k^\top \x^n) \right] \\
&= \lambda \w_k + \frac{1}{N} \sum_{n=1}^N \left[ -y_k^n \x^n + \sigma(\w_k^\top \x^n) \x^n \right]
\end{align*}

However, this method does not focus on ensuring the top few labels (\ie positive labels) are accurately modelled 
and also ignores the correlations between labels.



\subsubsection{Instance weighting with logistic loss}
Another baseline is to weight the loss for positive/negative labels with different constants, for example,
the loss of example $(\x, \y)$ given weights $\w$ can be
$$
\LCal(\x, \y; \w) = \frac{1}{K_+} \sum_{i:y_i=1} \ell_+(\w_i^\top \x) + \frac{1}{K_-} \sum_{j:y_j=0} \ell_-(\w_j^\top \x),
$$
where $K_+$ and $K_-$ are the number of positive and negative labels in $\y$ respectively. \\
If we use the logistic loss 
$$
\ell(v) = \log(1 + e^{-v})
$$
then
\begin{align*}
\ell_+(v) &= \ell(+1 \cdot v) = \log(1 + e^{-v}) \\
\ell_-(v) &= \ell(-1 \cdot v) = \log(1 + e^v)
\end{align*}
Thus the objective is
\begin{align*}
J(\w) 
&= \frac{\lambda}{2} \w^\top \w + \frac{1}{N} \sum_{n=1}^N \left(
   \frac{1}{K_+^n} \sum_{i:y_i^n=1} \log \left( 1 + \exp(-\w_i^\top \x^n) \right) + 
   \frac{1}{K_-^n} \sum_{j:y_j^n=0} \log \left( 1 + \exp(\w_j^\top \x^n) \right) \right) \\
&= \frac{\lambda}{2} \w^\top \w + \frac{1}{N} \sum_{k=1}^K \sum_{n=1}^N \left[
   \frac{\llb y_k^n = 1 \rrb}{K_+^n} \log \left( 1 + \exp(-\w_k^\top \x^n) \right) + 
   \frac{\llb y_k^n = 0 \rrb}{K_-^n} \log \left( 1 + \exp(\w_k^\top \x^n) \right) \right] 
\end{align*}
Where $K_+^n$ and $K_-^n$ are the number of positive and negative labels in $\y^n$ respectively. \\
Note that $\w^\top \w = \sum_{k=1}^K \w_k^\top \w_k$, the derivative of $\w_k, \, k \in \{1,\dots,K\}$ is
$$
\frac{\partial J(\w)} {\partial \w_k} = \lambda \w_k + \frac{1}{N} \sum_{n=1}^N \frac{\partial \LCal(\x^n, \y^n; \w)} {\partial \w_k},
$$
where 
$$
\frac{\partial \LCal(\x^n, \y^n; \w)} {\partial \w_k} =
\begin{cases}
\frac{-\x^n} {K_+^n (1 + \exp( \w_k^\top \x^n))}, & \text{if} \ y_k^n = 1 \\
\frac{ \x^n} {K_-^n (1 + \exp(-\w_k^\top \x^n))}, & \text{if} \ y_k^n = 0 \\
\end{cases}
$$
as a result,
$$
\frac{\partial J(\w)} {\partial \w_k} = \lambda \w_k + \frac{1}{N} \sum_{n=1}^N (a_n + b_n) \x^n,
$$
where
\begin{align*}
a_n &= \frac{-\llb y_k^n = 1 \rrb} {K_+^n (1 + \exp( \w_k^\top \x^n))} \\
b_n &= \frac{ \llb y_k^n = 0 \rrb} {K_-^n (1 + \exp(-\w_k^\top \x^n))}
\end{align*}




\subsubsection{Rank loss}
\label{sssec:rank}

Given a convex surrogate of the indicator function $\ell(\cdot)$, 
the rank loss of an example $(\x, \y)$ is
\begin{equation*}
\LCal(\x, \y; \w) = \frac{1}{K_+} \sum_{i: y_i = 1} \frac{1}{K_-} \sum_{j: y_j = 0} \ell(\w_i^\top \x - \w_j^\top \x),
\end{equation*}
where $K_+$ and $K_-$ is the number of positive and negative labels in $\y$ respectively, 
and $\ell(\cdot)$ used here is (log loss)
\begin{equation*}
\ell(v) = \log(1 + \exp(-v)).
\end{equation*}

Then the objective with rank loss is
\begin{equation}
\label{eq:obj_rank}
J(\w) = \frac{\lambda}{2} \w^\top \w + \frac{1}{N} \sum_{n=1}^N \frac{1}{K_+^n \cdot K_-^n} \sum_{i:y_i^n=1} \sum_{j:y_j^n=0} 
        \log \left( 1 + \exp \left( - \left( \w_i^\top \x^n - \w_j^\top \x^n \right) \right) \right).
\end{equation}
where $K_+^n$ and $K_-^n$ is the number of positive and negative labels in the $n$-th example respectively.

%The gradient of the weight vector $\w_i$ for positive labels and $\w_j$ for negative labels are
%\begin{align}
%%\label{eq:grad_rank_pos}
%\frac{\partial J(\w)}{\partial \w_i} & = \lambda \cdot \w_i + \frac{1}{N} \sum_{n=1}^N \frac{1}{K_+^n \cdot K_-^n} \sum_{j:y_j=0} 
%                                         \frac{-\x\pb{n}} {1 + \exp(\w_i^\top \x\pb{n} - \w_j^\top \x\pb{n})} \\
%\frac{\partial J(\w)}{\partial \w_j} & = \lambda \cdot \w_j + \frac{1}{N} \sum_{n=1}^N \frac{1}{K_+^n \cdot K_-^n} \sum_{i:y_i=1} 
%                                         \frac{\x\pb{n}} {1 + \exp(\w_i^\top \x\pb{n} - \w_j^\top \x\pb{n})}.
%\end{align}

To compute the derivative of weight vector $\w_k, \, k \in \{1,\cdots,K\}$, we note that $\w^\top \w = \sum_{k=1}^K \w_k^\top \w_k$ and 
\begin{equation}
\label{eq:grad_of_obj}
\frac{\partial J(\w)} {\partial \w_k} = \lambda \w_k + \frac{1}{N} \sum_{n=1}^N \frac{\partial \LCal(\x^n, \y^n; \w)} {\partial \w_k}.
\end{equation}
We further note that 
\begin{equation}
\label{eq:grad_decomp}
\frac{\partial \LCal(\x, \y; \w)} {\partial \w_k} =
\begin{cases}
\frac{\partial \LCal_+(\x, \y; \w)} {\partial \w_k} = \frac{1}{K_+ K_-} \underset{j:y_j=0}{\sum} \, \frac{-\x} {1 + \exp(\w_k^\top \x - \w_j^\top \x)}, 
    & \text{if} \ y_k=1 \\
\frac{\partial \LCal_-(\x, \y; \w)} {\partial \w_k} = \frac{1}{K_+ K_-} \underset{i:y_i=1}{\sum} \, \frac{\x} {1 + \exp(\w_i^\top \x - \w_k^\top \x)},
    & \text{if} \ y_k=0
\end{cases}
\end{equation}
which can be summarised as
\begin{equation}
\label{eq:grad_of_loss}
\frac{\partial \LCal(\x^n, \y^n; \w)} {\partial \w_k} =
\frac{\partial \LCal_+(\x^n, \y^n; \w)} {\partial \w_k} \llb y_k^n=1 \rrb +
\frac{\partial \LCal_-(\x^n, \y^n; \w)} {\partial \w_k} \llb y_k^n=0 \rrb.
\end{equation}
%
By Eq.~(\ref{eq:grad_of_obj}), (\ref{eq:grad_decomp}) and~(\ref{eq:grad_of_loss}), we have
\begin{equation}
\label{eq:grad_k}
\frac{\partial J(\w)} {\partial \w_k} = \lambda \w_k + \sum_{n=1}^N \frac{\x^n}{N K_+^n K_-^n} \left(
\underset{j:y_j^n=0}{\sum} \, \frac{-\llb y_k^n=1 \rrb} {1 + \exp(\w_k^\top \x^n - \w_j^\top \x^n)} +
\underset{i:y_i^n=1}{\sum} \, \frac{ \llb y_k^n=0 \rrb} {1 + \exp(\w_i^\top \x^n - \w_k^\top \x^n)} \right).
\end{equation}
%
We can rewrite Eq.~(\ref{eq:grad_k}) as
$$
\frac{\partial J(\w)} {\partial \w_k} = \lambda \w_k + \sum_{n=1}^N (a_n + b_n) \x^n
$$
where
\begin{align*}
a_n &= \frac{1}{N K_+^n K_-^n} \underset{j:y_j^n=0}{\sum} \, \frac{-\llb y_k^n=1 \rrb} {1 + \exp(\w_k^\top \x^n - \w_j^\top \x^n)} \\
b_n &= \frac{1}{N K_+^n K_-^n} \underset{i:y_i^n=1}{\sum} \, \frac{ \llb y_k^n=0 \rrb} {1 + \exp(\w_i^\top \x^n - \w_k^\top \x^n)}
\end{align*}



\subsubsection{p-classification loss}
\label{sssec:pclass}

The p-classification loss~\cite{ertekin2011equivalence} of example $(\x, \y)$ given weights $\w$ is
\begin{equation}
\label{eq:loss_pclass}
\LCal(\x, \y; \w) = \frac{1}{K_+} \sum_{i:y_i=1} \ell_+(\w_i^\top \x) + \frac{1}{K_-} \sum_{j:y_j=0} \ell_-(\w_j^\top \x),
\end{equation}
where $K_+$ and $K_-$ are defined as before, 
and for constant $p \gg 1$,
\begin{equation}
\begin{aligned}
\ell_+(v) & = \exp(-v), \\
\ell_-(v) & = \frac{1}{p} \exp(pv).
\end{aligned}
\end{equation}
When $p \to +\infty$, the above loss is equivalent to the top-push loss (with exponential surrogate).

Then the objective with p-classification loss is
\begin{align*}
J(\w) 
&= \frac{\lambda}{2} \w^\top \w + \frac{1}{N} \sum_{n=1}^N \left( 
   \frac{1}{K_+^n} \sum_{i:y_i^n=1} \exp(-\w_i^\top \x^n) + 
   \frac{1}{K_-^n} \sum_{j:y_j^n=0} \frac{1}{p} \exp(p \cdot \w_j^\top \x^n) \right) \\
&= \frac{\lambda}{2} \w^\top \w + \frac{1}{N} \sum_{k=1}^K \sum_{n=1}^N \left[
   \frac{\llb y_k^n = 1 \rrb}{K_+^n} \exp(-\w_k^\top \x^n) + 
   \frac{\llb y_k^n = 0 \rrb}{p K_-^n} \exp(p \cdot \w_k^\top \x^n) \right]
\end{align*}
where $K_+^n$ and $K_-^n$ are defined as before.


%The gradient of the weight vector $\w_i$ for positive labels and $\w_j$ for negative labels are
%\begin{align}
%\label{eq:grad_rank_pos}
%\frac{\partial J(\w)}{\partial \w_i} & = \lambda \cdot \w_i + \frac{1}{N} \sum_{n=1}^N \frac{-\x\pb{n}}{K_+^n} \exp(-\w_i^\top \x\pb{n}), \\
%\frac{\partial J(\w)}{\partial \w_j} & = \lambda \cdot \w_j + \frac{1}{N} \sum_{n=1}^N \frac{\x\pb{n}}{K_-^n} \exp(p \cdot \w_j^\top \x\pb{n}).
%\end{align}

Similar to Section~\ref{sssec:rank}, we can compute the derivative of weight vector $\w_k, \, k \in \{1,\cdots,K\}$ as follows:
$$
\frac{\partial J(\w)} {\partial \w_k} = \lambda \w_k + \frac{1}{N} \sum_{n=1}^N \frac{\partial \LCal(\x^n, \y^n; \w)} {\partial \w_k}.
$$
where
\begin{equation}
\frac{\partial \LCal(\x^n, \y^n; \w)} {\partial \w_k} =
\begin{cases}
\frac{-\x^n}{K_+} \exp(-\w_k^\top \x^n),  & \text{if} \ y_k^n=1 \\
\frac{ \x^n}{K_-} \exp(p\w_k^\top \x^n),  & \text{if} \ y_k^n=0
\end{cases}
\end{equation}
as a result,
\begin{equation}
\label{eq:grad_pclass}
\frac{\partial J(\w)} {\partial \w_k} = \lambda \w_k + \sum_{n=1}^N (a_n + b_n) \x^n,
\end{equation}
where
\begin{align*}
a_n &= \frac{-\llb y_k^n=1 \rrb} {N K_+^n} \exp( -\w_k^\top \x^n), \\
b_n &= \frac{ \llb y_k^n=0 \rrb} {N K_-^n} \exp(p \w_k^\top \x^n).
\end{align*}



\subsubsection{Top-push loss}
\label{sssec:tpush}

The top-push loss~\cite{li2014top} of example $(\x, \y)$ given weights $\w$ is
\begin{equation*}
%\label{eq:tpush_loss}
\LCal(\x, \y; \w) = \frac{1}{K_+} \sum_{i:y_i=1} \llb \w_i^\top \x \le \underset{j:y_j=0}{\max} \, \w_j^\top \x \rrb,
\end{equation*}
where $K_+$ is the number of positive labels in $\y$.

Let $\ell(\cdot)$ be a convex surrogate of the indicator function, by Eq.~(\ref{eq:obj}) our optimisation objective is
\begin{equation*}
%\label{eq:tpush_obj}
J(\w) = \frac{\lambda}{2} \w^\top \w + \frac{1}{N} \sum_{n=1}^N 
        \frac{1}{K_+^n} \sum_{i:y_i^n=1} \ell \left( \w_i^\top \x^n - \underset{j:y_j^n=0}{\max} \, \w_j^\top \x^n \right),
\end{equation*}
where $K_+^n$ is the number of positive labels in $\y^n$.

The objective~(\ref{eq:tpush_obj}) is hard to optimise due to the inner maximisation,
we can either approximate the inner maximisation or resort to optimise the dual problem of the objective.
