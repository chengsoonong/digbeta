\appendix
\section{Supplementary material to Sequence Recommendation via Structured Prediction}
\label{sec:supplement}

\subsection{List Viterbi}
\label{sec:listviterbi-supp}

We require a top-$k$ decoding of a sequence for three situations:
\begin{enumerate}
  \item To perform top-$k$ inference during the prediction phase
  \item To perform loss-augmented inference that includes avoiding all $n_i$ ground truth
    trajectories corresponding to the $i$th example.
  \item To find a decoding that does not have loops, that is to find a path
    and not a walk (with subtours).
\end{enumerate}

There are two general approaches for generalising Viterbi decoding to when we require $k$
sequences to be decoded: by maintaining $k$ paths through the trellis while decoding; or by
careful book keeping of the best $k-1$ paths through the trellis found so far and avoiding them.
We choose the latter approach as it is more memory efficient.

The list Viterbi approach~\cite{nilsson2001sequentially,seshadri1994list} maintains
a heap (\ie priority queue) of potential solutions, which are then checked for the desired property (for example
whether there are subtours). Once the requisite number of trajectories with the desired
property are found, the algorithm terminates (for example once one trajectory without subtours
is found). The heap is initialised by running forward-backward
(Algorithm~\ref{alg:forward-backward})
followed by Viterbi (Algorithm~\ref{alg:viterbi}).

\begin{algorithm}[htbp]
\caption{Forward-backward procedure~\cite{rabiner1989tutorial}}
\label{alg:forward-backward}
\begin{algorithmic}[1]
  \STATE $\forall p_j \in \mathcal{P},~ \alpha_t(p_j) =
          \begin{cases}
          0,~ t = 1 \\
          \max_{p_i \in \mathcal{P}} \left\{ \alpha_{t-1}(p_i) + \mathbf{w}_{ij}^\top \Psi_{ij}(\mathbf{x}, p_i, p_j) +
          \mathbf{w}_j^\top \Psi_j(\mathbf{x}, p_j) \right\},~ t=2,\dots,l
          \end{cases}$

  \STATE $\forall p_i \in \mathcal{P},~ \beta_t(p_i) =
          \begin{cases}
          0,~ t = l \\
          \max_{p_j \in \mathcal{P}} \left\{ \mathbf{w}_{ij}^\top \Psi_{ij}(\mathbf{x}, p_i, p_j) +
          \mathbf{w}_j^\top \Psi_j(\mathbf{x}, p_j) + \beta_{t+1}(p_j) \right\},~ t = l-1,\dots,1
          \end{cases}$

  %\STATE $\forall p_i \in \mathcal{P},~ f_t(p_i) = \alpha_t(p_i) + \beta_t(p_i),~ t = 1,\dots,K$
  \STATE $\forall p_i, p_j \in \mathcal{P},~ f_{t,t+1}(p_i, p_j) = \alpha_t(p_i) + \mathbf{w}_{ij}^\top \Psi_{ij}(\mathbf{x}, p_i, p_j) +
                                \mathbf{w}_j^\top \Psi_j(\mathbf{x}, p_j) + \beta_{t+1}(p_j),~ t = 1,\dots,l-1$
\end{algorithmic}
\end{algorithm}

\begin{algorithm}[htbp]
\caption{Viterbi}
\label{alg:viterbi}
\begin{algorithmic}[1]
  \STATE $y_t^1 = \begin{cases}
                  s,~ t = 1 \\
  %                \argmax_{p \in \mathcal{P}} \left\{ f_{1,2}(s, p) \right\},~ t = 2, \\
                  \argmax_{p \in \mathcal{P}} \left\{ f_{t-1,t}(y_{t-1}^1, p) \right\},~ t = 2,\dots,l
                  \end{cases}$

  %\STATE $r^1 = \max_{p \in \mathcal{P}} \left\{ f_K(p) \right\}~~~ \triangleright$ $r^1$ is the score/priority of $\mathbf{y}^1$
  \STATE $r^1 = \max_{p \in \mathcal{P}} \left\{ \alpha_{l}(p) \right\}~~~ \triangleright$ $r^1$ is the score/priority of $\mathbf{y}^1$
\end{algorithmic}
\end{algorithm}

Given an existing heap containing potential trajectories,
list Viterbi maintains a set of POIs $S$ to exclude, which is updated
sequentially by considering the front sequence of the heap.

Recall that for trajectory recommendation we are given the query $\mathbf{x}=(s, l)$, where
$s$ is the starting POI from the set of POIs $\mathcal{P}$,
and $l$ is the desired length of the trajectory.
We assume the score function is of the form $\mathbf{w}^\top \Psi$ where $\Psi$ is the joint
feature vector. $\mathbf{w}$ could be the value of the weight in the current iteration in training,
or the learned weight vector during prediction.

\begin{algorithm}[htbp]
\caption{The list Viterbi algorithm for top-$1$ prediction~\cite{nilsson2001sequentially,seshadri1994list}}
\label{alg:listviterbi}
\begin{algorithmic}[1]
\STATE \textbf{Input}: $\mathbf{x}=(s, l),~ \mathcal{P},~ \mathbf{w},~ \Psi$
%\STATE Initialise score matrices $\alpha,~ \beta,~ f_t,~ f_{t, t+1}$, a max-heap $H,~ k=0$.
\STATE Initialise score matrices $\alpha,~ \beta,~ f_{t, t+1}$, a max-heap $H$, result set $R$, $k=0$.
\STATE $\triangleright$ Do the forward-backward procedure~\cite{rabiner1989tutorial}
\STATE $\triangleright$ Identify the best (scored) trajectory $\mathbf{y}^1=(y_1^1,\dots,y_l^1)$
  with Viterbi. This may be a trajectory that violates the desired condition.
\STATE $H.\textit{push}\left(r^1,~ (\mathbf{y}^1, \textsc{nil}, \emptyset) \right)$
\STATE Set $R=\emptyset$, the list of trajectories to be returned.
\WHILE{$H \ne \emptyset$ \textbf{and} $k < \,|\mathcal{P}|^{l-1} - \prod_{t=2}^l (|\mathcal{P}|-t+1) + 1$}
    \STATE $r^k,~ (\mathbf{y}^k, I, S) = H.\textit{pop}()~~~ \triangleright$
           $r^k$ is the score of $\mathbf{y}^k=(y_1^k,\dots,y_l^k)$, $I$ is the partition index, and $S$ is the exclude set
    \STATE $k = k + 1$
    \STATE Add $\mathbf{y}^k$ to $R$ if it satisfies the desired property
    \RETURN $R$ if it contains the required number of trajectories
    \STATE $\bar{I} = \begin{cases}
                      2,~ I = \textsc{nil} \\
                      I,~ \text{otherwise}
                      \end{cases}$

    \FOR{$t = \bar{I},\dots,l$}
        \STATE $\bar{S} = \begin{cases}
                          S \cup \{ y_t^k \},& t = \bar{I} \\
                          \{ y_t^k \},& \text{otherwise}
                          \end{cases}$

        \STATE $\bar{y}_j = \begin{cases}
                            y_j^k,& j=1,\dots,t-1 \\
                            %\argmax_{p \in \mathcal{P} \setminus \textit{new\_exclude\_set}} f_{t-1,t}(y_{t-1}^k, p),~ j=t \\
                            \argmax_{p \in \mathcal{P} \setminus \bar{S}} \left\{ f_{t-1,t}(y_{t-1}^k, p) \right\},& j=t \\
                            \argmax_{p \in \mathcal{P}} \left\{ f_{j-1, j}(\bar{y}_{j-1}, p) \right\},& j=t+1,\dots,l
                \end{cases}$
        \STATE $\bar{r} = \begin{cases}
                          f_{t-1,t}(y_{t-1}^k, \bar{y}_t),&I = \textsc{nil} \\
                          r^k + f_{t-1,t}(y_{t-1}^k, \bar{y}_t) - f_{t-1,t}(y_{t-1}^k, y_t^k), &\text{otherwise}
                          \end{cases}$

        $H.\textit{push}\left(\bar{r}, (\bar{\mathbf{y}}, t, \bar{S}) \right)$
    \ENDFOR
\ENDWHILE
\end{algorithmic}
\end{algorithm}

\clearpage
\subsection{Photo Trajectory Dataset}
\label{sec:feature}

In the interests of reproducibility we present further details of our empirical experiment.

The POI and query specific features extracted from trajectories are shown in Table~\ref{tab:poifeature},
features that describe the transition preference between different POIs are shown in Table~\ref{tab:tranfeature}.

\begin{table*}[ht]
\caption{Features of POI $p$ with respect to query $(s,l)$}
\label{tab:poifeature}
\centering


\setlength{\tabcolsep}{10pt} % tweak the space between columns
\begin{tabular}{l|l} \hline
\textbf{Feature}       & \textbf{Description} \\ \hline
\texttt{category}      & one-hot encoding of the category of $p$ \\
\texttt{neighbourhood} & one-hot encoding of the POI cluster that $p$ resides in \\
\texttt{popularity}    & logarithm of POI popularity of $p$ \\
\texttt{nVisit}        & logarithm of the total number of visit by all users at $p$ \\
\texttt{avgDuration}  & logarithm of the average visit duration at $p$ \\
\hline
%\texttt{nOccurrence}            & the number of times $p$ occurred in a trajectory that satisfies the query \\ DON'T know given new query

\texttt{trajLen}           & trajectory length $l$, i.e., the number of POIs required \\
\texttt{sameCatStart}      & $1$ if the category of $p$ is the same as that of $s$, $-1$ otherwise \\
\texttt{sameNeighbourhoodStart} & $1$ if $p$ resides in the same POI cluster as $s$, $-1$ otherwise \\
\texttt{diffPopStart}    & real-valued difference in POI popularity of $p$ from that of $s$ \\
\texttt{diffNVisitStart}        & real-valued difference in the total number of visit at $p$ from that at $s$ \\
\texttt{diffDurationStart}  & real-valued difference in average duration at $p$ from that at $s$ \\
\texttt{distStart}          & distance between $p$ and $s$, calculated using the Haversine formula \\
\hline
\end{tabular}
\end{table*}



\begin{table}[ht]
\caption{POI features used to estimate the (feature-wise) transition probabilities}
\label{tab:tranfeature}
\centering
\setlength{\tabcolsep}{2pt} % tweak the space between columns
\begin{tabular}{l|l} \hline
\textbf{Feature}       & \textbf{Description} \\ \hline
\texttt{category}      & category of POI \\
\texttt{neighbourhood} & the cluster that a POI resides in \\
\texttt{popularity}    & (discretised) popularity of POI \\
\texttt{nVisit}        & (discretised) total number of visit at POI \\
\texttt{avgDuration}  & (discretised) average duration at POI \\ \hline
\end{tabular}
\end{table}


\clearpage
\subsection{Evaluation procedures}
\label{sec:metric}

To evaluate the performance of a certain recommendation algorithm,
we need to measure the similarity (or loss) given prediction $\hat{\mathbf{y}}$
and ground truth $\mathbf{y}$.

\begin{itemize}
  \item F$_1$ score on points~\cite{ijcai15}, where we care about the set of correctly recommended POIs.
      Let $\texttt{set}(\mathbf{y})$ denote the set of POIs in trajectory $\mathbf{y}$, F$_1$ score on points is defined as
\begin{equation*}
F_1(\mathbf{y}, \hat{\mathbf{y}}) = \frac{2  P_{\textsc{point}}  R_{\textsc{point}}}{P_{\textsc{point}} + R_{\textsc{point}}}
%~~\text{where}~
%P_{\textsc{point}} = \frac{| \texttt{set}(\hat{\mathbf{y}}) \cap \texttt{set}(\mathbf{y}) |}{| \texttt{set}(\hat{\mathbf{y}}) |}~\text{and}~
%R_{\textsc{point}} = \frac{| \texttt{set}(\hat{\mathbf{y}}) \cap \texttt{set}(\mathbf{y}) |}{| \texttt{set}(\mathbf{y}) |}.
\end{equation*}
where $P_\textsc{point}$, $R_\textsc{point}$ are respectively the precision and recall for points in $\hat\y$ and $\y$.
If $| \hat{\mathbf{y}} | = | \mathbf{y} |$, this metric is just the unordered Hamming loss,
i.e., Hamming loss between two binary indicator vectors of size $| \mathcal{P} |$.

\item F$_1$ score on pairs~\cite{cikm16paper}, where we care about the set of correctly predicted POI pairs,
\begin{equation*}
\text{pairs-F}_1(\mathbf{y}, \hat{\mathbf{y}}) = \frac{2 P_{\textsc{pair}} R_{\textsc{pair}}}{P_{\textsc{pair}} + R_{\textsc{pair}}}
%~~\text{where}~
%P_{\textsc{pair}} = \frac{N_c} {| \texttt{set}(\hat{\mathbf{y}}) | (| \texttt{set}(\hat{\mathbf{y}}) | - 1) / 2}~\text{and}~
%R_{\textsc{pair}} = \frac{N_c} {| \texttt{set}(\mathbf{y}) | (| \texttt{set}(\mathbf{y}) | - 1) / 2},
\end{equation*}
where $P_\textsc{point}$, $R_\textsc{point}$ are respectively the precision and recall for all possible pairs of $\hat\y$ and $\y$.
\eat{
and $N_c = \sum_{j=1}^{| \mathbf{y} | - 1} \sum_{k=j+1}^{| \mathbf{y} |} \llb y_j \prec_{\bar{\mathbf{y}}} y_k \rrb$,
here $y_j \prec_{\bar{\mathbf{y}}} y_k$ denotes that POI $y_j$ appears before POI $y_k$ in trajectory $\bar{\mathbf{y}}$.
We define pairs-F$_1 = 0$ when $N_c = 0$.
}

\end{itemize}

However, if we cast a trajectory $\mathbf{y} = (y_1,\dots,y_l)$ as a ranking of POIs in $\mathcal{P}$,
where $y_k$ has a rank $| \mathcal{P} | - k + 1$ and any other POI $p \notin \mathbf{y}$ has a rank $0$ ($0$ is an arbitrary choice).
We can make use of ranking evaluation metrics such as Kendall's $\tau$ by taking care of ties in ranks.

\subsection{Kendall's $\tau$ with ties}

Given a prediction $\hat{\mathbf{y}} = (\hat{y}_1, \hat{y}_2, \dots, \hat{y}_l)$ and ground truth $\mathbf{y} = (y_1, y_2, \dots, y_l)$,
for a specific ordering of POIs $(p_1, p_2, \dots, p_{|\mathcal{P}|})$,
we produce two ranks according to $\mathbf{y}$ and $\hat{\mathbf{y}}$,
\begin{align*}
r_i       &= \sum_{j=1}^l (| \mathcal{P} | - j + 1)  \llb p_i = y_j \rrb,~
i = 1, \dots, | \mathcal{P} | \\
\hat{r}_i &= \sum_{j=1}^l (| \mathcal{P} | - j + 1)  \llb p_i = \hat{y}_j \rrb,~
i = 1, \dots, | \mathcal{P} |
\end{align*}
where POIs not in $\mathbf{y}$ will have a rank of $0$ in $r$ and similarly in $\hat{r}$.
Then we have
\begin{align*}
C &= \frac{1}{2} \sum_{i,j} \left(\llb r_i < r_j \rrb  \llb \hat{r}_i < \hat{r}_j \rrb +
     \llb r_i > r_j \rrb  \llb \hat{r}_i > \hat{r}_j \rrb \right), \\
D &= \frac{1}{2} \sum_{i,j} \left(\llb r_i < r_j \rrb  \llb \hat{r}_i > \hat{r}_j \rrb +
     \llb r_i > r_j \rrb  \llb \hat{r}_i < \hat{r}_j \rrb \right), \\
T_{\mathbf{y}} &= \frac{1}{2} \sum_{i \ne j} \llb r_i = r_j \rrb \\
               &= \frac{1}{2} \sum_{i \ne j} \llb r_i = 0 \rrb  \llb r_j = 0 \rrb \\
               &= \frac{1}{2} \left( |\mathcal{P}| - l \right) \left( |\mathcal{P}| - l - 1 \right), \\
T_{\hat{\mathbf{y}}} &= \frac{1}{2} \sum_{i \ne j} \llb \hat{r}_i = \hat{r}_j \rrb \\
                     &= \frac{1}{2} \sum_{i \ne j} \llb \hat{r}_i = 0 \rrb  \llb \hat{r}_j = 0 \rrb \\
                     &= \frac{1}{2} \left( |\mathcal{P}| - l \right) \left( |\mathcal{P}| - l - 1 \right), \\
T_{\mathbf{y},\hat{\mathbf{y}}} &= \frac{1}{2} \sum_{i \ne j} \llb r_i = r_j \rrb  \llb \hat{r}_i = \hat{r}_j \rrb \\
                                &= \frac{1}{2} \sum_{i \ne j} \llb r_i = 0 \rrb  \llb r_j = 0 \rrb
                                   \llb \hat{r}_i = 0 \rrb  \llb \hat{r}_j = 0 \rrb.
\end{align*}
Kendall's $\tau$ (version $b$)~\cite{kendall1945,agresti2010analysis} is
\begin{equation*}
\tau_b(\mathbf{y}, \hat{\mathbf{y}}) = \frac{C - D}{\sqrt{(C + D + T) (C + D + U)}},
\end{equation*}
where $T = T_{\mathbf{y}} - T_{\mathbf{y},\hat{\mathbf{y}}}$ and $U = T_{\hat{\mathbf{y}}} - T_{\mathbf{y},\hat{\mathbf{y}}}$.

Furthermore, F$_1$ score on points can be written as
\begin{equation*}
F_1(\mathbf{y}, \hat{\mathbf{y}}) = \frac{1}{l} \sum_i \llb r_i > 0 \rrb  \llb \hat{r}_i > 0 \rrb,
\end{equation*}
and F$_1$ score on pairs can be written as
\begin{align*}
& \text{pairs-F}_1(\mathbf{y}, \hat{\mathbf{y}}) \\
&= \left( \frac{1}{2} \sum_{i,j} \llb r_i < r_j \rrb  \llb r_i > 0 \rrb \llb \hat{r}_i < \hat{r}_j \rrb  \llb \hat{r}_i > 0 \rrb \right. \\
&  \left. ~+ \frac{1}{2} \sum_{i,j} \llb r_i > r_j \rrb  \llb r_j > 0 \rrb \llb \hat{r}_i > \hat{r}_j \rrb  \llb \hat{r}_j > 0 \rrb \right)
   \cdot \frac{1}{l(l-1)/2} \\
&= \frac{\sum_{i,j} \llb r_i < r_j \rrb  \llb r_i > 0 \rrb \llb \hat{r}_i < \hat{r}_j \rrb  \llb \hat{r}_i > 0 \rrb +
         \sum_{i,j} \llb r_i > r_j \rrb  \llb r_j > 0 \rrb \llb \hat{r}_i > \hat{r}_j \rrb  \llb \hat{r}_j > 0 \rrb}
        {l(l-1)}
\end{align*}


% topk evaluation table
\input{tab_topk}


% histogram of #ground truth
\begin{figure}[t]
	\centering
	\includegraphics[width=\columnwidth]{hist_query.pdf}
	\caption{Histograms of the number of trajectories per query.}
	\label{fig:hist}
\end{figure}


% histogram of trajectory length
\begin{figure}[t]
	\centering
	\includegraphics[width=\columnwidth]{hist_length.pdf}
	\caption{Histograms of the length of trajectories.}
	\label{fig:hist}
\end{figure}

\clearpage

% topk evaluation plots
%\includepdf[pages={1-},scale=0.75]{plots.pdf}
\includepdf[pages={1-}]{plot_topk.pdf}


slack variable $\xi_i$ is the \emph{hinge loss} for the prediction of the $i$-th example~\cite{tsochantaridis2005large},
\begin{equation*}
%%\label{eq:hingeloss}
%%\resizebox{1.1\linewidth}{!}{
%%\begin{minipage}{\linewidth}
%\begin{align*}
\xi_i = \max \left( 0, \, 
        \max_{\bar{\y} \in \mathcal{Y}}
        \left\{ \Delta(\y^{(i)}, \bar{\y}) + \w^\top \Psi(\x^{(i)}, \bar{\y}) \right\} - \w^\top \Psi(\x^{(i)}, \y^{(i)}) \right).
%\end{align*}
%%\end{minipage}
%%}
\end{equation*}

we can use \emph{one} slack variable to represent the sum of the $N$ hinge losses in (\ref{eq:nslack_ml}),
which leads to %this $1$-slack formulation,
\begin{equation*}
%%\label{eq:1slack_ml}
\resizebox{0.9\linewidth}{!}{$
\begin{aligned}
\min_{\w, \, \xi \ge 0} ~\frac{1}{2} \w^\top \w + C \xi, ~~s.t.~ \frac{1}{N} \left( \sum_{i,j} \w^\top \Psi(\x^{(i)}, \y^{(ij)}) - \w^\top \Psi(\x^{(i)}, \bar{\y}^{(i)}) \right) 
  \ge \frac{1}{N} \sum_{i,j} \Delta(\y^{(ij)}, \bar{\y}^{(i)}) - \xi.
\end{aligned}
$}
\end{equation*}

