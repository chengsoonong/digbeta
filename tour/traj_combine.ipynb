{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trajectory Recommendation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "% matplotlib inline\n",
    "\n",
    "import os, re, sys, time, pickle, tempfile\n",
    "import math, random, itertools, scipy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.optimize as sop\n",
    "from joblib import Parallel, delayed\n",
    "from numba import jit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "random.seed(987654321) # control random choice when splitting training/testing set\n",
    "np.random.seed(987654321)\n",
    "ranksvm_dir = '$HOME/work/ranksvm'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_progress(cnt, total):\n",
    "    \"\"\"\n",
    "    Display a progress bar.\n",
    "    \"\"\"\n",
    "    assert(cnt > 0 and total > 0 and cnt <= total)\n",
    "    length = 80\n",
    "    ratio = cnt / total\n",
    "    n = int(length * ratio)\n",
    "    sys.stdout.write('\\r[%-80s] %d%%' % ('-'*n, int(ratio*100)))\n",
    "    sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_dir = 'data/data-ijcai15'\n",
    "#fvisit = os.path.join(data_dir, 'userVisits-Osak.csv')\n",
    "#fcoord = os.path.join(data_dir, 'photoCoords-Osak.csv')\n",
    "#fvisit = os.path.join(data_dir, 'userVisits-Glas.csv')\n",
    "#fcoord = os.path.join(data_dir, 'photoCoords-Glas.csv')\n",
    "#fvisit = os.path.join(data_dir, 'userVisits-Edin.csv')\n",
    "#fcoord = os.path.join(data_dir, 'photoCoords-Edin.csv')\n",
    "fvisit = os.path.join(data_dir, 'userVisits-Toro.csv')\n",
    "fcoord = os.path.join(data_dir, 'photoCoords-Toro.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "suffix = fvisit.split('-')[-1].split('.')[0]\n",
    "fseqpart = os.path.join(data_dir, 'seqPart-rank-' + suffix + '.pkl')\n",
    "fF1data = os.path.join(data_dir, 'F1-rank-' + suffix + '.pkl')\n",
    "fmlikeseq = os.path.join(data_dir, 'mlike-seq-' + suffix + '.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "visits = pd.read_csv(fvisit, sep=';')\n",
    "coords = pd.read_csv(fcoord, sep=';')\n",
    "assert(visits.shape[0] == coords.shape[0])\n",
    "traj = pd.merge(visits, coords, on='photoID') # merge data frames according to column 'photoID'\n",
    "#traj.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>#photo</th>\n",
       "      <th>#photo/user</th>\n",
       "      <th>#poi</th>\n",
       "      <th>#seq</th>\n",
       "      <th>#seq/user</th>\n",
       "      <th>#user</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Toro</th>\n",
       "      <td>39419</td>\n",
       "      <td>28.257348</td>\n",
       "      <td>29</td>\n",
       "      <td>6057</td>\n",
       "      <td>4.341935</td>\n",
       "      <td>1395</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      #photo  #photo/user  #poi  #seq  #seq/user  #user\n",
       "Toro   39419    28.257348    29  6057   4.341935   1395"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_photo = traj['photoID'].unique().shape[0]\n",
    "num_user = traj['userID'].unique().shape[0]\n",
    "num_poi = traj['poiID'].unique().shape[0]\n",
    "num_seq = traj['seqID'].unique().shape[0]\n",
    "pd.DataFrame({'#photo': num_photo, '#user': num_user, '#poi': num_poi, '#seq': num_seq, \\\n",
    "              '#photo/user': num_photo/num_user, '#seq/user': num_seq/num_user}, index=[str(suffix)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Compute POI Information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute POI (Longitude, Latitude) as the average coordinates of the assigned photos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "poi_coords = traj[['poiID', 'photoLon', 'photoLat']].groupby('poiID').mean()\n",
    "poi_coords.reset_index(inplace=True)\n",
    "poi_coords.rename(columns={'photoLon':'poiLon', 'photoLat':'poiLat'}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract POI category and visiting frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "poi_catfreq = traj[['poiID', 'poiTheme', 'poiFreq']].groupby('poiID').first()\n",
    "poi_catfreq.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>poiTheme</th>\n",
       "      <th>poiFreq</th>\n",
       "      <th>poiLon</th>\n",
       "      <th>poiLat</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>poiID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sport</td>\n",
       "      <td>3506</td>\n",
       "      <td>-79.379243</td>\n",
       "      <td>43.643183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sport</td>\n",
       "      <td>609</td>\n",
       "      <td>-79.418634</td>\n",
       "      <td>43.632772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sport</td>\n",
       "      <td>688</td>\n",
       "      <td>-79.380045</td>\n",
       "      <td>43.662175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sport</td>\n",
       "      <td>3056</td>\n",
       "      <td>-79.389290</td>\n",
       "      <td>43.641297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Cultural</td>\n",
       "      <td>986</td>\n",
       "      <td>-79.392396</td>\n",
       "      <td>43.653662</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       poiTheme  poiFreq     poiLon     poiLat\n",
       "poiID                                         \n",
       "1         Sport     3506 -79.379243  43.643183\n",
       "2         Sport      609 -79.418634  43.632772\n",
       "3         Sport      688 -79.380045  43.662175\n",
       "4         Sport     3056 -79.389290  43.641297\n",
       "6      Cultural      986 -79.392396  43.653662"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "poi_all = pd.merge(poi_catfreq, poi_coords, on='poiID')\n",
    "poi_all.set_index('poiID', inplace=True)\n",
    "poi_all.head()\n",
    "#poi_all.to_csv(fpoi, index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Construct Travelling Sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userID</th>\n",
       "      <th>seqID</th>\n",
       "      <th>poiID</th>\n",
       "      <th>arrivalTime</th>\n",
       "      <th>departureTime</th>\n",
       "      <th>#photo</th>\n",
       "      <th>poiDuration(sec)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10007579@N00</td>\n",
       "      <td>1</td>\n",
       "      <td>30</td>\n",
       "      <td>1346844688</td>\n",
       "      <td>1346844688</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10012675@N05</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>1142731848</td>\n",
       "      <td>1142732445</td>\n",
       "      <td>4</td>\n",
       "      <td>597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10012675@N05</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>1142916492</td>\n",
       "      <td>1142916492</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10012675@N05</td>\n",
       "      <td>4</td>\n",
       "      <td>13</td>\n",
       "      <td>1319327174</td>\n",
       "      <td>1319332848</td>\n",
       "      <td>9</td>\n",
       "      <td>5674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10014440@N06</td>\n",
       "      <td>5</td>\n",
       "      <td>24</td>\n",
       "      <td>1196128621</td>\n",
       "      <td>1196128878</td>\n",
       "      <td>3</td>\n",
       "      <td>257</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         userID  seqID  poiID  arrivalTime  departureTime  #photo  \\\n",
       "0  10007579@N00      1     30   1346844688     1346844688       1   \n",
       "1  10012675@N05      2      6   1142731848     1142732445       4   \n",
       "2  10012675@N05      3      6   1142916492     1142916492       1   \n",
       "3  10012675@N05      4     13   1319327174     1319332848       9   \n",
       "4  10014440@N06      5     24   1196128621     1196128878       3   \n",
       "\n",
       "   poiDuration(sec)  \n",
       "0                 0  \n",
       "1               597  \n",
       "2                 0  \n",
       "3              5674  \n",
       "4               257  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_all = traj[['userID', 'seqID', 'poiID', 'dateTaken']].copy().groupby(['userID', 'seqID', 'poiID'])\\\n",
    "          .agg([np.min, np.max, np.size])\n",
    "seq_all.columns = seq_all.columns.droplevel()\n",
    "seq_all.reset_index(inplace=True)\n",
    "seq_all.rename(columns={'amin':'arrivalTime', 'amax':'departureTime', 'size':'#photo'}, inplace=True)\n",
    "seq_all['poiDuration(sec)'] = seq_all['departureTime'] - seq_all['arrivalTime']\n",
    "seq_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userID</th>\n",
       "      <th>seqLen</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>seqID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10007579@N00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10012675@N05</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10012675@N05</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10012675@N05</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>10014440@N06</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             userID  seqLen\n",
       "seqID                      \n",
       "1      10007579@N00       1\n",
       "2      10012675@N05       1\n",
       "3      10012675@N05       1\n",
       "4      10012675@N05       1\n",
       "5      10014440@N06       1"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_stats = seq_all[['userID', 'seqID', 'poiID']].copy().groupby(['userID', 'seqID']).agg(np.size)\n",
    "seq_stats.reset_index(inplace=True)\n",
    "seq_stats.rename(columns={'poiID':'seqLen'}, inplace=True)\n",
    "seq_stats.set_index('seqID', inplace=True)\n",
    "seq_stats.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_seq(seqid, seq_all):\n",
    "    seqi = seq_all[seq_all['seqID'] == seqid].copy()\n",
    "    seqi.sort_values(by=['arrivalTime'], ascending=True, inplace=True)\n",
    "    return seqi['poiID'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calc_poi_info(seqid_set, seq_all, poi_all):\n",
    "    poi_info = seq_all[seq_all['seqID'].isin(seqid_set)][['poiID', 'poiDuration(sec)']].copy()\n",
    "    poi_info = poi_info.groupby('poiID').agg([np.mean, np.size])\n",
    "    poi_info.columns = poi_info.columns.droplevel()\n",
    "    poi_info.reset_index(inplace=True)\n",
    "    poi_info.rename(columns={'mean':'avgDuration(sec)', 'size':'popularity'}, inplace=True)\n",
    "    poi_info.set_index('poiID', inplace=True)\n",
    "    poi_info['poiTheme'] = poi_all.loc[poi_info.index, 'poiTheme']\n",
    "    poi_info['poiLon'] = poi_all.loc[poi_info.index, 'poiLon']\n",
    "    poi_info['poiLat'] = poi_all.loc[poi_info.index, 'poiLat']\n",
    "    return poi_info.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Train vs. Test Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Leave-one-out (the latest one?) for each user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "query_id_dict = dict()  # (start, end, length) --> qid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "seq_stats = seq_stats[seq_stats['seqLen'] > 2]\n",
    "seq_stats = seq_stats[seq_stats['seqLen'] < 10]\n",
    "seqid_set_ = seq_stats.index.tolist()\n",
    "seq_dict = dict()\n",
    "for seqid in seqid_set_:\n",
    "    seq = extract_seq(seqid, seq_all)\n",
    "    key = str(seq)\n",
    "    if key in seq_dict: seq_dict[key].append(seqid)\n",
    "    else: seq_dict[key] = [seqid]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "333"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(seqid_set_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "seqid_set_exp = [seq_dict[x][0] for x in sorted(seq_dict.keys())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "290"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(seqid_set_exp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a *query* (in IR terminology) using tuple (start POI, end POI, #POI) ~~user ID.~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "seqs_exp = [extract_seq(seqid, seq_all) for seqid in seqid_set_exp]\n",
    "keys = [(seq[0], seq[-1], len(seq)) for seq in seqs_exp]\n",
    "cnt = 0\n",
    "for key in keys:\n",
    "    if key not in query_id_dict:   # (start, end, length) --> qid\n",
    "        query_id_dict[key] = cnt\n",
    "        cnt += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "230\n"
     ]
    }
   ],
   "source": [
    "print(len(query_id_dict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Personalised Ranking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "POI Features used for ranking:\n",
    "1. `popularity`: POI popularity\n",
    "2. `avgDuration`: average POI visit duration\n",
    "3. `sameCatStart`: 1 if POI category is the same as that of `startPOI`, -1 otherwise\n",
    "4. `sameCatEnd`: 1 if POI category is the same as that of `endPOI`, -1 otherwise\n",
    "5. `distStart`: distance (haversine formula) from `startPOI`\n",
    "6. `distEnd`: distance from `endPOI`\n",
    "7. `seqLen`: trajectory length (copy from query)\n",
    "8. `diffPopStart`: difference in POI popularity from `startPOI`\n",
    "9. `diffPopEnd`: difference in POI popularity from `endPOI`\n",
    "10. `diffDurationStart`: difference in average POI visit duration from the actual duration spent at `startPOI`\n",
    "11. `diffDurationEnd`: difference in average POI visit duration from the actual duration spent at `endPOI`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Features aggregated from a number of trajectories:\n",
    "1. Compute POI `popularity` and average visit `duration` using all trajectories from training and querying set,\n",
    "1. Use the same features that computed above for the test set, except the distance based features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute distance between two POIs using [Haversine formula](http://en.wikipedia.org/wiki/Great-circle_distance)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calc_dist(longitude1, latitude1, longitude2, latitude2):\n",
    "    \"\"\"Calculate the distance (unit: km) between two places on earth\"\"\"\n",
    "    # convert degrees to radians\n",
    "    lon1 = math.radians(longitude1)\n",
    "    lat1 = math.radians(latitude1)\n",
    "    lon2 = math.radians(longitude2)\n",
    "    lat2 = math.radians(latitude2)\n",
    "    radius = 6371.0088 # mean earth radius is 6371.009km, en.wikipedia.org/wiki/Earth_radius#Mean_radius\n",
    "    # The haversine formula, en.wikipedia.org/wiki/Great-circle_distance\n",
    "    dlon = math.fabs(lon1 - lon2)\n",
    "    dlat = math.fabs(lat1 - lat2)\n",
    "    return 2 * radius * math.asin(math.sqrt(\\\n",
    "               (math.sin(0.5*dlat))**2 + math.cos(lat1) * math.cos(lat2) * (math.sin(0.5*dlon))**2 ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training data are generated as follows:\n",
    "1. each input tuple $(\\text{startPOI}, \\text{endPOI}, \\text{#POI})$ form a `query` (in IR terminology).\n",
    "1. the label of a specific POI is the number of presence of that POI in a specific `query`, excluding the presence as $\\text{startPOI}$ or $\\text{endPOI}$.\n",
    "1. for each `query`, the label of all absence POIs from trajectories of that `query` in training set got a label 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dimension of training data matrix is `#(qid, poi)` by `#feature`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gen_train_df(seqid_set, seq_all, poi_info, query_id_dict):\n",
    "    columns = ['poiID', 'label', 'queryID', 'popularity', 'avgDuration(sec)', 'sameCatStart', 'sameCatEnd', \\\n",
    "               'distStart', 'distEnd', 'seqLen', 'diffPopStart', 'diffPopEnd', 'diffDurationStart', 'diffDurationEnd']\n",
    "    seqid_set = sorted(set(seqid_set))\n",
    "    train_seqs = [extract_seq(seqid, seq_all) for seqid in seqid_set]\n",
    "    for seq in train_seqs:\n",
    "        assert(len(seq) > 2)\n",
    "    \n",
    "    qid_set = sorted(set([query_id_dict[(seq[0], seq[-1], len(seq))] for seq in train_seqs]))\n",
    "    poi_set = sorted(set(poi_info.index.tolist()))\n",
    "    qid_poi_pair = list(itertools.product(qid_set, poi_set)) # Cartesian product of qid_set and poi_set\n",
    "    \n",
    "    df_ = pd.DataFrame(data=np.zeros((len(qid_poi_pair), len(columns)), dtype= np.float), columns=columns)\n",
    "    \n",
    "    query_id_rdict = dict()\n",
    "    for k, v in query_id_dict.items(): \n",
    "        query_id_rdict[v] = k  # qid --> (start, end, length)\n",
    "    \n",
    "    for i in range(df_.index.shape[0]):\n",
    "        qid = qid_poi_pair[i][0]\n",
    "        poi = qid_poi_pair[i][1]\n",
    "        (p0, pN, seqLen) = query_id_rdict[qid]\n",
    "        lon0 = poi_info.loc[p0, 'poiLon']; lat0 = poi_info.loc[p0, 'poiLat']\n",
    "        lonN = poi_info.loc[pN, 'poiLon']; latN = poi_info.loc[pN, 'poiLat']\n",
    "        lon = poi_info.loc[poi, 'poiLon']; lat = poi_info.loc[poi, 'poiLat']\n",
    "        pop = poi_info.loc[poi, 'popularity']; cat = poi_info.loc[poi, 'poiTheme']\n",
    "        duration = poi_info.loc[poi, 'avgDuration(sec)']\n",
    "        idx = df_.index[i]\n",
    "        df_.loc[idx, 'poiID'] = poi\n",
    "        df_.loc[idx, 'queryID'] = qid\n",
    "        df_.loc[idx, 'popularity'] = pop\n",
    "        df_.loc[idx, 'avgDuration(sec)'] = duration\n",
    "        df_.loc[idx, 'sameCatStart'] = 1 if cat == poi_info.loc[p0, 'poiTheme'] else -1\n",
    "        df_.loc[idx, 'sameCatEnd']   = 1 if cat == poi_info.loc[pN, 'poiTheme'] else -1\n",
    "        df_.loc[idx, 'distStart'] = 0 if poi == p0 else calc_dist(lon, lat, lon0, lat0)\n",
    "        df_.loc[idx, 'distEnd']   = 0 if poi == pN else calc_dist(lon, lat, lonN, latN)\n",
    "        df_.loc[idx, 'seqLen'] = seqLen\n",
    "        df_.loc[idx, 'diffPopStart'] = 0 if poi == p0 else pop - poi_info.loc[p0, 'popularity']\n",
    "        df_.loc[idx, 'diffPopEnd']   = 0 if poi == pN else pop - poi_info.loc[pN, 'popularity']\n",
    "        df_.loc[idx, 'diffDurationStart'] = 0 if poi == p0 else duration - poi_info.loc[p0, 'avgDuration(sec)']\n",
    "        df_.loc[idx, 'diffDurationEnd']   = 0 if poi == pN else duration - poi_info.loc[pN, 'avgDuration(sec)']\n",
    "    \n",
    "    # set label\n",
    "    df_.set_index(['queryID', 'poiID'], inplace=True)\n",
    "    for seq in train_seqs:\n",
    "        qid = query_id_dict[(seq[0], seq[-1], len(seq))]\n",
    "        for poi in seq[1:-1]:  # do NOT count if the POI is startPOI/endPOI\n",
    "            df_.loc[(qid, poi), 'label'] += 1\n",
    "    \n",
    "    df_.reset_index(inplace=True)\n",
    "    return df_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sanity check: \n",
    "- different POIs have different features for the same query trajectory\n",
    "- the same POI get different features for different query-id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test data are generated the same way as training data, except that the labels of testing data (unknown) could be arbitrary values as suggested in [libsvm FAQ](http://www.csie.ntu.edu.tw/~cjlin/libsvm/faq.html#f431).\n",
    "The reported accuracy (by `svm-predict` command) is meaningless as it is calculated based on these labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dimension of training data matrix is `#poi` by `#feature` with one specific `query`, i.e. tuple $(\\text{startPOI}, \\text{endPOI}, \\text{#POI})$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gen_test_df(startPOI, endPOI, nPOI, poi_info, query_id_dict):\n",
    "    columns = ['poiID', 'label', 'queryID', 'popularity', 'avgDuration(sec)', 'sameCatStart', 'sameCatEnd', \\\n",
    "               'distStart', 'distEnd', 'seqLen', 'diffPopStart', 'diffPopEnd', 'diffDurationStart', 'diffDurationEnd']\n",
    "    key = (p0, pN, seqLen) = (startPOI, endPOI, nPOI)\n",
    "    assert(key in query_id_dict)\n",
    "    \n",
    "    poi_set = sorted(set(poi_info.index.tolist()))\n",
    "    df_ = pd.DataFrame(data=np.zeros((len(poi_set), len(columns)), dtype= np.float), columns=columns)\n",
    "    \n",
    "    qid = query_id_dict[key]\n",
    "    df_['queryID'] = qid\n",
    "    df_['label'] = np.random.rand(df_.shape[0]) # label for test data is arbitrary according to libsvm FAQ\n",
    "\n",
    "    lon0 = poi_info.loc[p0, 'poiLon']; lat0 = poi_info.loc[p0, 'poiLat']\n",
    "    lonN = poi_info.loc[pN, 'poiLon']; latN = poi_info.loc[pN, 'poiLat']\n",
    "    for i in range(df_.index.shape[0]):\n",
    "        poi = poi_set[i]\n",
    "        lon = poi_info.loc[poi, 'poiLon']; lat = poi_info.loc[poi, 'poiLat']\n",
    "        pop = poi_info.loc[poi, 'popularity']; cat = poi_info.loc[poi, 'poiTheme']\n",
    "        duration = poi_info.loc[poi, 'avgDuration(sec)']\n",
    "        idx = df_.index[i]\n",
    "        df_.loc[idx, 'poiID'] = poi \n",
    "        df_.loc[idx, 'popularity'] = pop\n",
    "        df_.loc[idx, 'avgDuration(sec)'] = duration\n",
    "        df_.loc[idx, 'sameCatStart'] = 1 if cat == poi_info.loc[p0, 'poiTheme'] else -1\n",
    "        df_.loc[idx, 'sameCatEnd']   = 1 if cat == poi_info.loc[pN, 'poiTheme'] else -1\n",
    "        df_.loc[idx, 'distStart'] = 0 if poi == p0 else calc_dist(lon, lat, lon0, lat0)\n",
    "        df_.loc[idx, 'distEnd']   = 0 if poi == pN else calc_dist(lon, lat, lonN, latN)\n",
    "        df_.loc[idx, 'seqLen'] = seqLen\n",
    "        df_.loc[idx, 'diffPopStart'] = 0 if poi == p0 else pop - poi_info.loc[p0, 'popularity']\n",
    "        df_.loc[idx, 'diffPopEnd']   = 0 if poi == pN else pop - poi_info.loc[pN, 'popularity']\n",
    "        df_.loc[idx, 'diffDurationStart'] = 0 if poi == p0 else duration - poi_info.loc[p0, 'avgDuration(sec)']\n",
    "        df_.loc[idx, 'diffDurationEnd']   = 0 if poi == pN else duration - poi_info.loc[pN, 'avgDuration(sec)']\n",
    "    return df_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sanity check: \n",
    "- different POIs have different features for the same query trajectory\n",
    "- the same POI get different features for different query-id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate a string for a training/test data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gen_data_str(df_):\n",
    "    columns = ['label', 'queryID', 'popularity', 'avgDuration(sec)', 'sameCatStart', 'sameCatEnd', 'distStart', \\\n",
    "               'distEnd', 'seqLen', 'diffPopStart', 'diffPopEnd', 'diffDurationStart', 'diffDurationEnd']\n",
    "    for col in columns:\n",
    "        assert(col in df_.columns)\n",
    "        \n",
    "    lines = []\n",
    "    for idx in df_.index:\n",
    "        slist = [str(df_.loc[idx, 'label'])]\n",
    "        slist.append(' qid:')\n",
    "        slist.append(str(int(df_.loc[idx, 'queryID'])))\n",
    "        for j in range(2, len(columns)):\n",
    "            slist.append(' ')\n",
    "            slist.append(str(j-1))\n",
    "            slist.append(':')\n",
    "            slist.append(str(df_.loc[idx, columns[j]]))\n",
    "        slist.append('\\n')\n",
    "        lines.append(''.join(slist))\n",
    "    return ''.join(lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ranking POIs using rankSVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RankSVM implementation in libsvm can be downloaded [here (zip file)](https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/ranksvm/libsvm-ranksvm-3.20.zip), please read `README.ranksvm` in the zip file for installation instructions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is a python wrapper of the `svm-train` or `train` and `svm-predict` or `predict` commands of rankSVM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# python wrapper of rankSVM\n",
    "class RankSVM:\n",
    "    def __init__(self, bin_dir, useLinear=False, debug=False):\n",
    "        dir_ = !echo $bin_dir  # deal with environmental variables in path\n",
    "        assert(os.path.exists(dir_[0]))\n",
    "        self.bin_dir = dir_[0]\n",
    "        \n",
    "        self.bin_train = 'svm-train'\n",
    "        self.bin_predict = 'svm-predict'\n",
    "        if useLinear:\n",
    "            self.bin_train = 'train'\n",
    "            self.bin_predict = 'predict'\n",
    "        \n",
    "        assert(isinstance(debug, bool))\n",
    "        self.debug = debug\n",
    "        \n",
    "        # create named tmp files for model and feature scaling parameters\n",
    "        self.fmodel = None\n",
    "        self.fscale = None\n",
    "        with tempfile.NamedTemporaryFile(delete=False) as fd: \n",
    "            self.fmodel = fd.name\n",
    "        with tempfile.NamedTemporaryFile(delete=False) as fd: \n",
    "            self.fscale = fd.name\n",
    "        \n",
    "        if self.debug:\n",
    "            print('model file:', self.fmodel)\n",
    "            print('feature scaling parameter file:', self.fscale)\n",
    "    \n",
    "    \n",
    "    def __del__(self):\n",
    "        # remove tmp files\n",
    "        if self.fmodel is not None and os.path.exists(self.fmodel):\n",
    "            os.unlink(self.fmodel)\n",
    "        if self.fscale is not None and os.path.exists(self.fscale):\n",
    "            os.unlink(self.fscale)\n",
    "    \n",
    "    \n",
    "    def train(self, train_df, cost=1):\n",
    "        # cost is parameter C in SVM\n",
    "        # write train data to file\n",
    "        ftrain = None\n",
    "        with tempfile.NamedTemporaryFile(mode='w+t', delete=False) as fd: \n",
    "            ftrain = fd.name\n",
    "            datastr = gen_data_str(train_df)\n",
    "            fd.write(datastr)\n",
    "        \n",
    "        # feature scaling\n",
    "        ftrain_scaled = None\n",
    "        with tempfile.NamedTemporaryFile(mode='w+t', delete=False) as fd: \n",
    "            ftrain_scaled = fd.name\n",
    "        result = !$self.bin_dir/svm-scale -s $self.fscale $ftrain > $ftrain_scaled\n",
    "        \n",
    "        if self.debug:\n",
    "            print('cost:', cost)\n",
    "            print('train data file:', ftrain)\n",
    "            print('feature scaled train data file:', ftrain_scaled)\n",
    "        \n",
    "        # train rank svm and generate model file, if the model file exists, rewrite it\n",
    "        #n_cv = 10  # parameter k for k-fold cross-validation, NO model file will be generated in CV mode\n",
    "        #result = !$self.bin_dir/svm-train -c $cost -v $n_cv $ftrain $self.fmodel\n",
    "        result = !$self.bin_dir/$self.bin_train -c $cost $ftrain_scaled $self.fmodel\n",
    "        if self.debug:\n",
    "            print('Training finished.')\n",
    "            for i in range(len(result)): print(result[i])\n",
    "\n",
    "        # remove train data file\n",
    "        os.unlink(ftrain)\n",
    "        os.unlink(ftrain_scaled)\n",
    "        \n",
    "    \n",
    "    def predict(self, test_df, poi_list):\n",
    "        \"\"\"\n",
    "        Row `i` in DataFrame `test_df` corresponds to the features of POI `poi_list[i]`\n",
    "        \"\"\"\n",
    "        assert(test_df.shape[0] == len(poi_list))\n",
    "        \n",
    "        if self.fmodel is None or not os.path.exists(self.fmodel):\n",
    "            print('Model should be trained before predicting')\n",
    "            return\n",
    "        \n",
    "        # write test data to file\n",
    "        ftest = None\n",
    "        with tempfile.NamedTemporaryFile(mode='w+t', delete=False) as fd: \n",
    "            ftest = fd.name\n",
    "            datastr = gen_data_str(test_df)\n",
    "            fd.write(datastr)\n",
    "                \n",
    "        # feature scaling\n",
    "        ftest_scaled = None\n",
    "        with tempfile.NamedTemporaryFile(delete=False) as fd: \n",
    "            ftest_scaled = fd.name\n",
    "        result = !$self.bin_dir/svm-scale -r $self.fscale $ftest > $ftest_scaled\n",
    "            \n",
    "        # generate prediction file\n",
    "        fpredict = None\n",
    "        with tempfile.NamedTemporaryFile(delete=False) as fd: \n",
    "            fpredict = fd.name\n",
    "            \n",
    "        if self.debug:\n",
    "            print('test data file:', ftest)\n",
    "            print('feature scaled test data file:', ftest_scaled)\n",
    "            print('predict result file:', fpredict)\n",
    "        \n",
    "            \n",
    "        # predict using trained model and write prediction to file\n",
    "        result = !$self.bin_dir/$self.bin_predict $ftest_scaled $self.fmodel $fpredict\n",
    "        if self.debug:\n",
    "            print('Predict result: %-30s  %s' % (result[0], result[1]))\n",
    "        \n",
    "        # generate prediction DataFrame from prediction file\n",
    "        poi_rank_df = pd.read_csv(fpredict, header=None)\n",
    "        poi_rank_df.rename(columns={0:'rank'}, inplace=True)\n",
    "        poi_rank_df['poiID'] = poi_list\n",
    "        poi_rank_df.set_index('poiID', inplace=True)\n",
    "        \n",
    "        # remove test file and prediction file\n",
    "        os.unlink(ftest)\n",
    "        os.unlink(ftest_scaled)\n",
    "        os.unlink(fpredict)\n",
    "        \n",
    "        return poi_rank_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Platt Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transform ranking scores (output of rankSVM) to a probability distribution using [Platt scaling](https://en.wikipedia.org/wiki/Platt_scaling)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code from paper `Lin, Hsuan-Tien; Lin, Chih-Jen; Weng, Ruby C. (2007).` \n",
    "[`A note on Platt's probabilistic outputs for support vector machines`](http://stocktrendresearch.googlecode.com/svn-history/r77/trunk/Paper/SVM_ANN/plattprob.pdf).\n",
    "`Machine Learning 68 (3): 267â€“276.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def platt_scaling(deci, label, prior1, prior0):\n",
    "    \"\"\"\n",
    "    Input parameters:\n",
    "    - deci = array of SVM decision values (of examples in training set)\n",
    "    - label = array of booleans: is the example labeled +1? (in training set)\n",
    "    - prior1 = number of positive examples (in training set)\n",
    "    - prior0 = number of negative examples (in training set)\n",
    "    Outputs:\n",
    "    - A, B = parameters of sigmoid\n",
    "    \"\"\"\n",
    "\n",
    "    # Parameter setting\n",
    "    maxiter = 100   # Maximum number of iterations\n",
    "    minstep = 1e-10 # Minimum step taken in line search\n",
    "    sigma = 1e-12   # Set to any value > 0\n",
    "\n",
    "    # Construct initial values: target support in array t, initial function value in fval\n",
    "    hiTarget = (prior1 + 1.0) / (prior1 + 2.0)\n",
    "    loTarget = 1 / (prior0 + 2.0)\n",
    "    len_ = prior1 + prior0  # Total number of data\n",
    "    t = np.zeros(len_, dtype=np.float)\n",
    "    for i in range(len_):\n",
    "        if label[i] == True: \n",
    "            t[i] = hiTarget\n",
    "        else:\n",
    "            t[i] = loTarget\n",
    "\n",
    "    A = 0.0\n",
    "    B = math.log((prior0 + 1.0) / (prior1 + 1.0))\n",
    "    fval = 0.0\n",
    "    for i in range(_len):\n",
    "        fApB = deci[i] * A + B\n",
    "        if fApB >= 0:\n",
    "            fval += t[i] * fApB + math.log(1 + math.exp(-fApB))\n",
    "        else:\n",
    "            fval += (t[i] - 1) * fApB + math.log(1 + math.exp(fApB))\n",
    "\n",
    "    for it in range(maxiter):\n",
    "        # Update Gradient and Hessian (use H' = H + sigma I)\n",
    "        h11 = h22 = sigma\n",
    "        h21 = g1 = g2 = 0.0\n",
    "        for i in range(len_):\n",
    "            fApB = deci[i] * A + B\n",
    "            if fApB >= 0:\n",
    "                p = math.exp(-fApB) / (1.0 + math.exp(-fApB))\n",
    "                q = 1.0 / (1.0 + math.exp(-fApB))\n",
    "            else:\n",
    "                p = 1.0 / (1.0 + math.exp(fApB))\n",
    "                q = math.exp(fApB) / (1.0 + math.exp(fApB))\n",
    "            d2 = p * q\n",
    "            h11 += deci[i] * deci[i] * d2\n",
    "            h22 += d2\n",
    "            h21 += deci[i] * d2\n",
    "            d1 = t[i] - p\n",
    "            g1 += deci[i] * d1\n",
    "            g2 += d1\n",
    "\n",
    "        # Stopping criteria\n",
    "        if math.fabs(g1) < 1e-5 and math.fabs(g2) < 1e-5:  \n",
    "            break\n",
    "\n",
    "        # Compute modified Newton directions\n",
    "        det = h11 * h22 - h21 * h21\n",
    "        dA = -(h22 * g1 - h21 * g2) / det\n",
    "        dB = -(-h21 * g1 + h11 * g2) / det\n",
    "        gd = g1 * dA + g2 * dB\n",
    "\n",
    "        # Line search\n",
    "        stepsize = 1\n",
    "        while stepsize >= minstep:\n",
    "            newA = A + stepsize * dA\n",
    "            newB = B + stepsize * dB\n",
    "            newf = 0.0\n",
    "            for i in range(len_):\n",
    "                fApB = deci[i] * newA + newB\n",
    "                if fApB >= 0:\n",
    "                    newf += t[i] * fApB + math.log(1 + math.exp(-fApB))\n",
    "                else:\n",
    "                    newf += (t[i] - 1) * fApB + math.log(1 + math.exp(fApB))\n",
    "\n",
    "            # Sufficient decrease satisfied\n",
    "            if newf < fval + 0.0001 * stepsize * gd:\n",
    "                A = newA\n",
    "                B = newB\n",
    "                fval = newf\n",
    "                break\n",
    "            else:\n",
    "                stepsize /= 2.0\n",
    "\n",
    "        if stepsize < minstep:\n",
    "            print('Line search fails')\n",
    "            break\n",
    "\n",
    "    if it >= maxiter:\n",
    "        print('Reaching maximum iterations')\n",
    "\n",
    "    return (A, B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Factorise Personalised Transition Counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Personalised MC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Personalised Recommendation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation metrics without considering visiting order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calc_F1score(seq_act, seq_rec)#, includeStartEnd=True):\n",
    "    assert(len(seq_act) > 2)\n",
    "    assert(len(seq_rec) > 2)\n",
    "\n",
    "    act_set = set(seq_act)\n",
    "    rec_set = set(seq_rec)\n",
    "    intersect = act_set & rec_set\n",
    "    \n",
    "    recall    = len(intersect) / len(act_set) #if includeStartEnd else (len(intersect)-2) / (len(act_set)-2)\n",
    "    precision = len(intersect) / len(rec_set) #if includeStartEnd else (len(intersect)-2) / (len(rec_set)-2)\n",
    "    #F1score   = 0 if abs(precision + recall) < 1e-6 else 2. * precision * recall / (precision + recall)\n",
    "    F1score   = 2. * precision * recall / (precision + recall)\n",
    "  \n",
    "    return F1score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.misc import comb\n",
    "from math import factorial\n",
    "def rand_guess(npoi, length):\n",
    "    assert(length <= npoi)\n",
    "    if length == npoi: return 1\n",
    "    N = npoi - 2\n",
    "    m = length - 2 # number of correct POIs\n",
    "    k = m\n",
    "    expected_F1 = 0\n",
    "    while k >= 0:\n",
    "        F1 = (k + 2) / length\n",
    "        prob = comb(m, k) * comb(N-m, m-k) / comb(N, m)\n",
    "        expected_F1 += prob * F1\n",
    "        k -= 1\n",
    "    return expected_F1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sanity check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rand_guess(20, 5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
