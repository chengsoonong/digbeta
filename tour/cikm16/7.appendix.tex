\newpage

\appendix

\urlstyle{tt}

\section{POI Categories}
We collected the POI categories in all of the five trajectory datasets,
their names with the corresponding icons are shown in figure \ref{fig:poicats}.

%\cheng{Report the proportion of recommended trajectories and actual trajectories with sub-tours, in the Appendix.}

\section{POIs sharing the same feature vector}
When a group of POIs sharing identical (discretised) features,
we distribute the probability uniformly among the POIs in the group as follows.

Firstly, the incoming (unnormalised) transition probability of the group computed by taking 
the Kronecker product is divided uniformly among POIs in the group,
which is equivalent to choose a POI in the group uniformly at random.

On the other hand, the outgoing (unnormalised) transition probability of each POI 
should be the same as that of the POI group, 
since in this case, the transition from any POI in the group to one outside the group 
represents an outgoing transition from the POI group.

Furthermore, the self-loop transition of the POI group represents transitions from any POI in the group
to any other POI in the same group, we distribute the self-loop transition probability uniformly 
among all these pairs.
In particular, suppose the (unnormalised) self-loop transition probability is $P_o$,
and the number of POIs in the group is $N_o$,
the transition probability from $p_i$ to $p_j$ in the same group is
\begin{displaymath}
    P(p_j | p_i) = \delta(i \ne j) \frac{P_o}{N_o - 1}, \forall 1 \le i, j \le N_o
\end{displaymath}
Where $\delta(i \ne j) = 1$ if $i \ne j$ and $0$ otherwise, which is the same constraint that no POI
self-loops are allowed, as described in Section~\ref{sec:transition}.

Lastly, we normalise each row of the (unnormalised) POI-POI transition matrix 
to form a valid (outgoing) probability distribution for each POI.


\section{POI Feature Distribution}
% Figures from the appendix
\begin{figure*}[t]
\includegraphics[width=\textwidth]{fig/feature_distro.pdf}
\caption{Distribution of POI popularity, the number of visit and visit duration}
\label{fig:distro}\captionmoveup
\end{figure*}


\section{POI Transitions}
\begin{figure*}[htbp]
\includegraphics[width=\textwidth]{fig/poi_transmat_all.png}
\caption{Transition matrices for five POI features: POI categories, neighborhood, popularity, number of visits, and visit duration. These statistics are from the Melbourne dataset. See Section~\ref{sec:transition} for descriptions.}
\label{fig:transmat}
\end{figure*}


\section{Markov}

\begin{algorithm}[t]
\caption{\textsc{Markov}: recommend trajectory with POI transitions}
%recommend trajectory by maximising likelihood}
\label{alg:markov}
\begin{algorithmic}[1]
\STATE \textbf{Input}: $\mathcal{P}, p_s, p_e, L$
\STATE \textbf{Output}: Trajectory $\mathcal{T} = (p_s, \cdots, p_e)$ with $L$ POIs
%\STATE Compute POI-POI transition matrix
\STATE Initialise score matrix $A$ and backtracking pointer $B$
\FOR{$p \in \mathcal{P}$}
    \STATE $A[2, p] = \log P(p|p_s)$
    \STATE $B[2, p] = p_s$
\ENDFOR
\FOR{$l=2$ to $L-1$}
    \FOR{$p \in \mathcal{P}$}
        \STATE \(\displaystyle A[l+1, p] = \max_{p' \in \mathcal{P}} \{ A[l, p'] + \log P(p|p') \} \)
        \STATE \(\displaystyle B[l+1, p] = \argmax_{p' \in \mathcal{P}} \{ A[l, p'] + \log P(p|p') \} \)
    \ENDFOR
\ENDFOR
% //trace back to find the actual path
\STATE $\mathcal{T} = \{p_e\}$, $l = L$, $p = \mathcal{T}.first$
\REPEAT
    \STATE Prepend $B[l, p]$ to $\mathcal{T}$
    \STATE $l = l - 1$, $p = \mathcal{T}.first$
\UNTIL{$l < 2$}
\RETURN $\mathcal{T}$
\end{algorithmic}
\end{algorithm}




\section{Implementation details}
We use the rankSVM implementation in libsvmtools \url{https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/},
and the Structured SVM implementation in PyStruct \url{https://pystruct.github.io/}.
Integer linear programming are solved using both Gurobi Optimizer \url{http://www.gurobi.com/} 
and lp\_solve \url{http://lpsolve.sourceforge.net/}.


\section{Avoid Peeking}
When working with machine learning algorithms, to make sure the reported performance is a good approximation
of the generalisation performance, it is critical to prevent information in test set from leaking into
training set.
Many algorithms shown in Table~\ref{tab:algsummary} leveraging both learning to rank and 
factorised POI-POI transition matrix,
e.g., \textsc{Rank+Markov}, \textsc{Rank+MarkovPath} and \textsc{StructuredSVM},
both of them need to be trained or parameters be estimated before being utilised in other algorithms.
POI features such as popularity, the number of visits and average visit duration are
determined by not only the POI itself but also trajectories in training set, 
let's call them aggregated features, as they are computed by aggregating a set of trajectories.
To make sure the prediction performance is reliable, 
it is very important to exclude trajectories in test set when computing aggregated features.
Unfortunately, it is quite easy, especially when utilising multiple levels of machine learning models,
to use all data, including those in test set, to compute aggregated features, 
and many researchers and practitioners did not realise the fact that 
some bits of information in test set were leaked into training set via these aggregated features.

%One may argue that many of these features will not change much when computed with or without data in test set,
%but in certain areas, such as aerodynamics, some decisions are very sensitive to the quantity of certain features.
%Nevertheless, the exact impact still needs further investigation.


\begin{figure}[t]
	\centering
	\includegraphics[width=0.6\columnwidth]{fig/poi_cats.pdf}
	\caption{POI Categories}
	\label{fig:poicats}
\end{figure}


\begin{table}[t]
\caption{POI features used to factorise POI-POI transition probabilities}
\label{tab:featuretran}
\centering
\setlength{\tabcolsep}{2pt} % tweak the space between columns
\begin{tabular}{l|l} \hline
\textbf{Feature}       & \textbf{Description} \\ \hline
\texttt{category}      & category of POI \\
\texttt{neighbourhood} & the cluster that a POI resides in \\
\texttt{popularity}    & (discretised) popularity of POI \\
\texttt{nVisit}        & (discretised) total number of visit at POI \\
\texttt{avgDuration}   & (discretised) average duration at POI \\ \hline
\end{tabular}
\end{table}


\begin{table*}[t]
\caption{Features of POI $p$ used in rankSVM given query $(p_s, p_e, L)$}
\label{tab:featurerank}
\centering
%\small
%\setlength{\tabcolsep}{2pt} % tweak the space between columns
%\begin{tabular}{l|p{0.7\columnwidth}} \hline
\begin{tabular}{l|l} \hline
\textbf{Feature}  & \textbf{Description} \\ \hline
\texttt{category}               & one-hot encoding of the category of $p$ \\
\texttt{neighbourhood}          & one-hot encoding of the POI cluster that $p$ resides in \\
\texttt{popularity}             & logarithm of POI popularity of $p$ \\
\texttt{nVisit}                 & logarithm of the total number of visit by all users at $p$ \\
\texttt{avgDuration}            & logarithm of the average duration at $p$ \\ \hline
\texttt{trajLen}                & trajectory length $L$, i.e., the number of POIs required \\
\texttt{sameCatStart}           & $1$ if the category of $p$ is the same as that of $p_s$, $-1$ otherwise \\
\texttt{sameCatEnd}             & $1$ if the category of $p$ is the same as that of $p_e$, $-1$ otherwise \\
\texttt{sameNeighbourhoodStart} & $1$ if $p$ resides in the same POI cluster as $p_s$, $-1$ otherwise \\
\texttt{sameNeighbourhoodEnd}   & $1$ if $p$ resides in the same POI cluster as $p_e$, $-1$ otherwise \\
\texttt{distStart}              & distance between $p$ and $p_s$, calculated using the Haversine formula \\
\texttt{distEnd}                & distance between $p$ and $p_e$, calculated using the Haversine formula \\
\texttt{diffPopStart}           & real-valued difference in POI popularity of $p$ from that of $p_s$ \\
\texttt{diffPopEnd}             & real-valued difference in POI popularity of $p$ from that of $p_e$ \\
\texttt{diffNVisitStart}        & real-valued difference in the total number of visit at $p$ from that at $p_s$ \\
\texttt{diffNVisitEnd}          & real-valued difference in the total number of visit at $p$ from that at $p_e$ \\
\texttt{diffDurationStart}      & real-valued difference in average duration at $p$ from that at $p_s$ \\
\texttt{diffDurationEnd}        & real-valued difference in average duration at $p$ from that at $p_e$ \\ \hline
\end{tabular}
\end{table*}


\section{RankSVM}
\label{appendix:ranksvm}

\subsection{Prediction}
Given a set of $M$ POIs $\mathcal{P} = \{p_1, \cdots, p_M\}$, 
and $N$ queries $\mathcal{Q} = \{q_1, \cdots, q_N\}$.
The ranking score for POI $p_m$ with respect to query $q_n$ is 
$S_{m,n} = \langle \mathbf{w_r}, \mathbf{\phi}(p_m, q_n) \rangle$,
where $\mathbf{w_r}$ is a vector of parameters,
and $\mathbf{\phi}(p_m, q_n)$ is the scaled feature vector for $p_m$ with respect to query $q_n$,
as described in Table~\ref{tab:featurerank}.

\subsection{Feature scaling}
We scale the features described in Table~\ref{tab:featurerank} to range $[-1, 1]$ using the same approach as that 
utilised by libsvm \url{http://www.csie.ntu.edu.tw/~cjlin/libsvm/},
i.e., we fit a linear function $f(x) = a x + b$ for feature $x$ such that the maximum value of $x$ maps to $1$ 
and the minimum value of $x$ map to $-1$.

\subsection{Training}
To estimate the parameters $\mathbf{w_r}$, we train a rankSVM with linear kernel and L$2$ loss:
%\begin{displaymath}
\begin{multline}
\min_{\mathbf{w_r}} \frac{1}{2} \mathbf{w_r}^T \mathbf{w_r} + \\ \hfill
                    C_r \sum_{(p_i, q_n), (p_j, q_n) \in \mathcal{P} \times \mathcal{Q}}
                    \max \left( 0, 1 - \mathbf{w_r}^T (\mathbf{\phi}(p_i, q_n) - \mathbf{\phi}(p_j, q_n)) \right)^2
\end{multline}
%\end{displaymath}
where $C_r > 0$ is the regularisation parameter.

The label of a training example is the number of occurrences of POI $p_m$ in trajectories grouped by query $q = (p_s, p_e, L)$,
without counting the occurrence of $p_m$ when it is the origin or destination of a trajectory.


\section{Transition probabilities}

In addition to information about each individual POI, a tour recommendation system would benefit
from capturing the likelihood of transitioning between different POIs. One option would be to
directly model the probability of going from one POI to another, but this has several weaknesses:
Such a model would be unable to handle a new POI (one that has not yet been visited).
Furthermore, even if we restrict ourselves to known POIs, there may be many locations which
are rarely visited, leading to significant challenges in estimating the probabilities from
empirical data.

We model POI transitions using a Markov chain with discrete factored states.
One difficulty of estimating the Markov chain is data sparsity, 
i.e., not enough transitions have been observed between every pair of POIs. 
We factorised the transition probability from POI $p_i$ to POI $p_j$ %is  according to
as a product of transition probabilities between pairs of individual POI features.
%as described in Table~\ref{tab:featuretran}.
POIs are grouped into $5$ clusters using K-means according to their geographical locations
to reflect their neighbourhood relations.
We directly model the transition between the category and neighbourhood of each POI as the conditional probability.
The popularity, total number of visits and the average visit duration are discretised by binning
them uniformly into $5$ discrete intervals on the log scale.
Figure~\ref{fig:transmat} visualises the transition matrices for individual POI features in Melbourne.

We compute the transition probabilities of the above individual POI features
using maximum likelihood estimation,
i.e., counting the number of transitions for each pair of features then normalising each row,
taking care of zeros by adding a small number $\epsilon$
%\footnote{In our experiments, $\epsilon = 1$.}
to each count before normalisation,
which results in a transition matrix for each of the above POI features.
%
Assuming independence between these features,
the transition probability $P(p_j | p_i)$ from $p_i$ to $p_j$ can be defined as the product
of the transition probabilities of each individual feature (and appropriately normalised),
with two additional constraints.
First we disallow self transitions by setting the probability of ($p_i$ to $p_i$) to zero.
Second we need to deal with the possibility that multiple POIs may share exactly the same feature vector.
When a group of POIs have identical (discretised) features, we distribute the probability uniformly among the POIs in the group. 
%More details of this procedure are provided in the Appendix.
The POI-POI transition matrix can be efficiently computed by taking the Kronecker product of
the transition matrices for the individual features and then updating it based on the constraints described above.

\section{Factorised POI-POI transitions}

\label{appendix:transition}
We factorise the transition probability from POI $p_i$ to POI $p_j$
according to $5$ POI features described in Table~\ref{tab:featuretran},
which result in a transition matrix for each individual POI feature.


\subsection{Structured SVM}
\label{sec:ssvm}
\secmoveup

As trajectory is a sequence of POI visits,
it is natural to model the recommended trajectory $\mathcal{T}$ with respect to query $q = (p_s, p_e, L)$
as a chain of $L$ variables, where each discrete variable has $|\mathcal{P}|$ states.
Structured prediction incorporates both the features of variables (unary features) and
the features of interactions between neighbouring variables (pairwise features) to make a
prediction,
\begin{displaymath}
    \mathcal{T}^* = \argmax_{\mathcal{T} \in \mathcal{P}^L} 
                    \sum_{j=1}^L \mathbf{w_u}^T \mathbf{\phi}_j \left( q, \mathcal{T}_j \right) +
                    \sum_{j=1}^{L-1} \mathbf{w_p}^T \mathbf{\phi}_{j, j+1} \left( q, \mathcal{T}_j, \mathcal{T}_{j+1} \right)
\end{displaymath}
where $\mathbf{\phi}_j$ are the unary features of the $j$-th variable and $\mathbf{\phi}_{j, j+1}$ are the pairwise features between
the $j$-th and $(j+1)$-th variables, $\mathbf{w_u}$ and $\mathbf{w_p}$ are the
parameters of unary and pairwise features respectively.

The unary and pairwise features are constructed from the POI ranking and POI-POI transitions.
In particular, unary features are defined as ranking probabilities (Equation~\ref{eq:poi-probability}).
Recall that we have a query consisting of start ($p_s$) and end ($p_e$) locations, which
we model as a 1-of-$K$ encoding in the unary features.
The pairwise features are defined from the transition probabilities $P(p_j | p_i)$ described in
Section~\ref{sec:transition}.
This method is called \textsc{StructuredSVM} in the experiments.

\eat{
In the settings of trajectory recommendation, the ranking probabilities of POIs
with respect to constraint $x = (p_s, p_e, L)$ were used to capture the unary features of individual variables
and the transition probabilities between POIs were utilized to capture the pairwise features
between the neighboring variables, in particular,
the unary features of the first and last variables are binary vectors
with true values at the corresponding POIs and false values anywhere else,
the unary features of the other $L-2$ variables are the ranking probabilities of POIs in $\mathcal{P}$, i.e.,
\begin{displaymath}
    \phi_j(x, y_j) = P_R(y_j | x), y_j \in \mathcal{P}
\end{displaymath}

Pairwise features between the $j$-th variable and the $(j+1)$-th variable $\phi_{j, j+1}$ was defined as
\begin{align*}
    \phi_{j, j+1}(x, y_j, y_{j+1}) &= \phi_{j-1, j}(x, y_{j-1}, y_j) \times M \\
                                 j &=2, \dots, L-2
\end{align*}
where $\phi_{j-1, j}$ is the pairwise features between the $(j-1)$-th and $j$-th variables,
and $M$ is the transition matrix between POIs described in Section~\ref{sec:transition} and
$M_{ij} = P(p_j | p_i)$.
In particular, the pairwise features between the first and the second variables is the
outgoing transition probabilities of the first variable,
and the pairwise features between the second-last and the last variables are a probability distribution
over all POIs in $\mathcal{P}$ where the probability mass is dominated by the variable corresponding to POI $p_e$,
all other POIs in $\mathcal{P} \setminus p_e$ are simply uniformly distributed.
}

% describe unary/pairwise potentials?

% describe SSVM training (1-slack formulation)
To estimate the parameters $\mathbf{w_u}$ and $\mathbf{w_p}$, we train a Structured Support Vector Machine
using the 1-slack formulation~\cite{ssvm09},
\begin{align*}
    \min_{\mathbf{w}, \xi \ge 0} ~~& \frac{1}{2} \mathbf{w}^T \mathbf{w} + C \xi \\
    s.t. ~~& \forall \left( \hat{\mathcal{T}}^{(1)}, \cdots, \hat{\mathcal{T}}^{(N)} \right) \in \mathscr{T}^N: \\
         ~~& \frac{1}{N} \mathbf{w}^T \sum_{i=1}^N \delta \left( \hat{\mathcal{T}}^{(i)} \right) \ge
             \frac{1}{N} \sum_{i=1}^N \Delta \left( \mathcal{T}^{(i)}, \hat{\mathcal{T}}^{(i)} \right) - \xi \\
%         ~~& \forall \hat{\mathcal{T}}^{(i)} \in \mathcal{P}^{|\mathcal{T}^{(i)}|}, i = 1, \cdots, N
\end{align*}
where $\mathbf{w} = [\mathbf{w_u}^T, \mathbf{w_p}^T]^T$ is the parameter vector,
$\mathcal{T}^{(i)}$ and $\hat{\mathcal{T}}^{(i)}$ are the $i$-th trajectory in training set
and its corresponding recommendation respectively.
$N$ is the training set size, $C$ is the regularisation parameter,
$\xi$ is the slack variable, and
\begin{displaymath}
    \delta \left( \hat{\mathcal{T}}^{(i)} \right) = \Psi \left( q^{(i)}, \mathcal{T}^{(i)} \right) - 
                                                    \Psi \left( q^{(i)}, \hat{\mathcal{T}}^{(i)} \right)
\end{displaymath}
where $q^{(i)}$ the query corresponds to the $i$-th trajectory in training set,
$\Psi \left( q^{(i)}, \mathcal{T}^{(i)} \right)$ is the joint feature vector which is a composite of unary and 
pairwise features of the $i$-th trajectory,
$\Delta \left( \mathcal{T}^{(i)}, \hat{\mathcal{T}}^{(i)} \right)$ is the loss associated with the $i$-th trajectory 
and its corresponding recommendation, and Hamming loss is used in this work.



\section{Structured SVM}
\label{appendix:ssvm}

To recommend a trajectory that starts from POI $p_s$, ends at POI $p_e$, and with $L$ POIs in total.
We build a chain with $L$ random variables (nodes) and $L-1$ directed edges between neighbouring variables,
each discrete variable has $|\mathcal{P}|$ states.
Both the first variable and the last variable are observed, i.e., they are $p_s$ and $p_e$ respectively,
all intermediate variables corresponds to POIs that should be recommended.


\subsection{Node Features}
\label{appendix:node}
Given a POI $p_m$ and a query $q_n$, the node features are the same as that described in Table~\ref{tab:featurerank}.
Features are scaled the same way as that described in Section~\ref{appendix:ranksvm}.


\subsection{Edge Features}
\label{appendix:edge}
Given an ordered pair of POIs $(p_i, p_j)$, 
the edge/pairwise features are built from the factorised transition probabilities 
according to features described in Table~\ref{tab:featuretran}:
\begin{enumerate}
\item The transition probability from the category of $p_i$ to the category of $p_j$.
\item The transition probability from the cluster with $p_i$ to the cluster with $p_j$.
\item The transition probability from the interval with the popularity of $p_i$ to the interval 
      with the popularity of $p_j$.
\item The transition probability from the interval with the number of visit at $p_i$ to the interval 
      with the number of visit at $p_j$.
\item The transition probability from the interval with the average duration at $p_i$ to the interval 
      with the average duration at $p_j$.
\end{enumerate}


\subsection{Prediction}
Given parameters for node and edge features $\mathbf{w_u}$ and $\mathbf{w_p}$, 
we can compute the score of $\mathcal{T}$ as follows:
\begin{displaymath}
S'(\mathcal{T}) = \sum_{j=1}^L     \langle \mathbf{w_u}, \mathbf{\phi}_j (\mathcal{T}_j, q) \rangle +
                  \sum_{j=1}^{L-1} \langle \mathbf{w_p}, \mathbf{\phi}_{j, j+1} (\mathcal{T}_j, \mathcal{T}_{j+1}) \rangle
\end{displaymath}
where $L = |\mathcal{T}|$ is the length of $\mathcal{T}$, 
$\mathbf{\phi}_j$ is the feature vector of the $j$-th node (variable) described in Section~\ref{appendix:node},
$q$ is the query corresponding to $\mathcal{T}$, i.e., $q = (\mathcal{T}_0, \mathcal{T}_L, L)$,
and $\mathbf{\phi}_{j, j+1}$ is the feature vector of the $j$-th edge, as described in Section~\ref{appendix:edge}.

One approach to recommend a trajectory with respect to query $q = (p_s, p_e, L)$ and parameters 
$\mathbf{w_u}$ and $\mathbf{w_p}$ is recommending the one with the maximum score, i.e.,
\begin{displaymath}
    \mathcal{T}^* = \argmax_{\mathcal{T} \in \mathcal{P}^L} S'(\mathcal{T})
\end{displaymath}


\subsection{Training}
To estimate the parameters $\mathbf{w_u}$ and $\mathbf{w_p}$, we train a Structured Support Vector Machine
using the 1-slack formulation,
\begin{align*}
    \min_{\mathbf{w}, \xi \ge 0} ~~& \frac{1}{2} \mathbf{w}^T \mathbf{w} + C \xi \\
    s.t. ~~& \forall \left( \bar{\mathcal{T}}^{(1)}, \cdots, \bar{\mathcal{T}}^{(N)} \right) \in \mathscr{T}^N: \\
         ~~& \frac{1}{N} \mathbf{w}^T \sum_{i=1}^N \delta \left( \bar{\mathcal{T}}^{(i)} \right) \ge
             \frac{1}{N} \sum_{i=1}^N \Delta \left( \mathcal{T}^{(i)}, \bar{\mathcal{T}}^{(i)} \right) - \xi
\end{align*}
where $\mathbf{w} = [\mathbf{w_u}^T, \mathbf{w_p}^T]^T$ is the parameter vector,
$C > 0$ is the regularisation parameter, $\xi$ is the slack variable, 
$N$ is the number of trajectories in training set, 
$\mathcal{T}^{(i)}$ is the $i$-th trajectory in training set, 
and its corresponding recommendation is $\bar{\mathcal{T}}^{(i)}$, and
\begin{displaymath}
    \delta \left( \bar{\mathcal{T}}^{(i)} \right) = \Psi \left( \mathcal{T}^{(i)}, q^{(i)} \right) - 
                                                    \Psi \left( \bar{\mathcal{T}}^{(i)}, q^{(i)} \right)
\end{displaymath}
where $q^{(i)}$ is the query corresponding to $\mathcal{T}^{(i)}$ and the joint feature vector of $\mathcal{T}^{(i)}$ is
\begin{displaymath}
\Psi \left( \mathcal{T}^{(i)}, q^{(i)} \right) = \left[
\left( \sum_{j=1}^{|\mathcal{T}^{(i)}|}   \mathbf{\phi}_{j}      \left( \mathcal{T}_{j}^{(i)}, q^{(i)} \right) \right)^T,
\left( \sum_{j=1}^{|\mathcal{T}^{(i)}|-1} \mathbf{\phi}_{j, j+1} \left( \mathcal{T}_{j}^{(i)}, \mathcal{T}_{j+1}^{(i)} \right) \right)^T 
\right]^T
\end{displaymath}
$\Delta \left( \mathcal{T}^{(i)}, \bar{\mathcal{T}}^{(i)} \right)$ is the loss associated with the $i$-th trajectory 
and its corresponding recommendation, and Hamming loss is used in our experiment.


\section{Experiment}
Trajectories are extracted using  geo-tagged photos in the Yahoo! Flickr Creative Commons 100M
(a.k.a. YFCC100M) dataset~\cite{thomee2016yfcc100m} as well as the Wikipedia web-pages of points-of-interests (POIs).
Photos are mapped to POIs according to their distances calculated using the Haversine formula~\cite{haversine},
the time a user arrived a POI is approximated by the time the first photo taken by the user at that POI,
similarly, the time a user left a POI is approximated by the time the last photo taken
by the user at that POI.
Furthermore, sequence of POI visits by a specific user are divided into several pieces according to
the time gap between consecutive POI visits, and the POI visits in each piece are connected in temporal order
to form a trajectory.
%%LX: said above, no need to repeat
%For details of extracting trajectories from geo-tagged photos, please refer to \cite{ht10, ijcai15}.


%To evaluate the performance of different trajectory recommendation algorithms,
%we employ the trajectory F$_1$\cite{ijcai15} to measure the POIs that are
%correctly recommended.
A commonly used metric for POI recommendation and evaluating trajectories is
the F$_1$ score on points.
Let $\mathcal{T}$ be the trajectory that was visited in the real world,
and $\hat{\cal T}$ be the recommended trajectory,
$\mathcal{P}_{\mathcal{T}}$ be the set of POIs visited in $\mathcal{T}$,
and $\mathcal{P}_{\hat{\mathcal{T}}}$ be the set of POIs visited in $\hat{\mathcal{T}}$,
we compute trajectory F$_1$ as the harmonic mean between the point-wise precision and recall.
%was defined as
\begin{displaymath}
F_1= \frac{2  P_{\textsc{point}}  R_{\textsc{point}}}
          {P_{\textsc{point}} + R_{\textsc{point}}}
\end{displaymath}
where
\vspace{-1.1em}
\begin{displaymath}%\eqmoveup
P_{\textsc{point}} = \frac{|\mathcal{P}_{\mathcal{T}} \cap \mathcal{P}_{\hat{\mathcal{T}}}|}
                          {|\hat{\mathcal{T}}|},
R_{\textsc{point}} = \frac{|\mathcal{P}_{\mathcal{T}} \cap \mathcal{P}_{\hat{\mathcal{T}}}|}
                          {|\mathcal{T}|}
\end{displaymath}

A perfect trajectory F$_1$ (i.e., F$_1 = 1$) means the POIs in
the recommended trajectory are exactly the same POIs as those in the ground truth,
and F$_1 = 0$ means that none of the POIs in the
real trajectory was recommended.

While trajectory F$_1$ is good at measuring whether POIs are correctly recommended,
it ignores the visiting order between POIs.
%Two trajectories with the exactly same set of POIs can be completely different to visitors
%given different routes.
An illustration is shown in Figure~\ref{fig:pairf1},
the solid grey lines represent the ground truth transitions that actually visited by travellers
and the dashed blue lines are the recommended trajectory by one of the approaches listed
in Table~\ref{tab:algsummary}. Both examples have a perfect F$_1$ score, but not a perfect
pairs-F$_1$ score due to the difference in POI sequencing.
%Even with the exactly same set of places, the visiting experiences are completely different because
%of the dramatically disparate routes.
%Enlightened by this observation,
We propose a new metric $\text{pairs-F}_1$ that takes into account
both the point identity and the visiting orders in a trajectory.
This is done by measuring the F$_1$ score of every pair of ordered POIs, whether they are adjacent or not.
\begin{displaymath}
\text{pairs-F}_1 = \frac{2 P_{\textsc{pair}} R_{\textsc{pair}}}
                       {P_{\textsc{pair}} + R_{\textsc{pair}}}
\end{displaymath}
where
\vspace{-2.0em}
\begin{displaymath}%\eqmoveup
~~
P_{\textsc{pair}} = \frac{N_c} {|\hat{\mathcal{T}}|(|\hat{\mathcal{T}}|-1) / 2}, ~
R_{\textsc{pair}} = \frac{N_c} {|\mathcal{T}|(|\mathcal{T}|-1) / 2}
\end{displaymath}
and $N_c$ is the number of ordered POI pairs $(p_j, p_k)$ that
appear in both the ground-truth and the recommended trajectories.
%satisfies the following
%conditions\footnote{We define pairs-F$_1=0$ if $N_c$ is $0$.}:
\begin{align*}
    (p_j \prec_{\mathcal{T}} p_k ~\land~ p_j \prec_{\hat{\mathcal{T}}} p_k)  ~\lor~
    (p_j \succ_{\mathcal{T}} p_k ~\land~ p_j \succ_{\hat{\mathcal{T}}} p_k) 
\end{align*}
with $p_j \ne p_k, ~p_j, p_k \in \mathcal{P}_{\mathcal{T}} \cap \mathcal{P}_{\hat{\mathcal{T}}}, ~1 \le j \ne k \le |\mathcal{T}|$.
Here $p_j \prec_{\mathcal{T}} p_k$ denotes POI $p_j$ was visited before POI $p_k$ in $\mathcal{T}$
and $p_j \succ_{\mathcal{T}} p_k$ denotes $p_j$ was visited after $p_k$ in $\mathcal{T}$.

Pairs-F$_1$ takes values between 0 and 1. A perfect pairs-F$_1$ (1.0) is achieved if and only if
both the POIs and their visiting orders in the
recommended trajectory are exactly the same as those in the ground truth.
Pairs-F$_1 = 0$ means none of the recommended POI pairs was actually visited in the real trajectory.


